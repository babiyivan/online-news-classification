{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tuwnlp.dataset import PropagandaDataset\n",
    "from tuwnlp.utils import Language\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:00, 5050.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language.EN LabelLevel.NARATIVES (200, 23)\n",
      "(200, 23)\n"
     ]
    }
   ],
   "source": [
    "dataset = PropagandaDataset(\"../data\", languages=[Language.EN], tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased'))\n",
    "train, val, test = torch.utils.data.random_split(dataset, [100, 50, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class CustomBertClassifier(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super(CustomBertClassifier, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\n",
    "            \"distilbert-base-multilingual-cased\", \n",
    "        )\n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.linear1 = nn.Linear(in_features=768, out_features=hidden_size, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features=512)\n",
    "        self.relu1 =  nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.8)\n",
    "        self.linear2 = nn.Linear(in_features=hidden_size, out_features=out_size, bias=False)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(num_features=512)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(**x)[0]\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = torch.max(x,1, keepdim=False).values\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "    def freeze_bert(self):\n",
    "        \"\"\"\n",
    "        Freezes the parameters of BERT so when BertWithCustomNNClassifier is trained\n",
    "        only the wieghts of the custom classifier are modified.\n",
    "        \"\"\"\n",
    "        for param in self.bert.named_parameters():\n",
    "            param[1].requires_grad=False\n",
    "    \n",
    "    def unfreeze_bert(self):\n",
    "        \"\"\"\n",
    "        Unfreezes the parameters of BERT so when BertWithCustomNNClassifier is trained\n",
    "        both the wieghts of the custom classifier and of the underlying BERT are modified.\n",
    "        \"\"\"\n",
    "        for param in self.bert.named_parameters():\n",
    "            param[1].requires_grad=True\n",
    "\n",
    "    def freeze_embeddings(self):\n",
    "        \"\"\"\n",
    "        Freezes the parameters of BERT so when BertWithCustomNNClassifier is trained\n",
    "        only the wieghts of the custom classifier are modified.\n",
    "        \"\"\"\n",
    "        for param in self.bert.embeddings.named_parameters():\n",
    "            param[1].requires_grad=False\n",
    "    \n",
    "    def unfreeze_embeddings(self):\n",
    "        \"\"\"\n",
    "        Unfreezes the parameters of BERT so when BertWithCustomNNClassifier is trained\n",
    "        both the wieghts of the custom classifier and of the underlying BERT are modified.\n",
    "        \"\"\"\n",
    "        for param in self.bert.embeddings.named_parameters():\n",
    "            param[1].requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "import wandb\n",
    "\n",
    "def training_step(dataloader, model, optimizer, loss_fn, freeze_bert = False, freeze_embeddings = False, runner = None):\n",
    "    \"\"\"Method to train the model\"\"\"\n",
    "    model.train()\n",
    "    model.freeze_bert() if freeze_bert else model.unfreeze_bert()\n",
    "    model.freeze_embeddings() if not freeze_bert and freeze_embeddings else model.unfreeze_bert()\n",
    "      \n",
    "    epoch_loss = 0\n",
    " \n",
    "    for x, y in tqdm(dataloader):        \n",
    "        output = model(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(output, y.float())\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val = loss.item()\n",
    "        if runner is not None:\n",
    "            runner.log({\"train/loss\": loss_val})\n",
    "\n",
    "def eval_step(dataloader, model, loss_fn, runner = None):\n",
    "    \"\"\"Method to eval the model\"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(dataloader):        \n",
    "            output = model(x)\n",
    "            loss = loss_fn(output, y.float())\n",
    "            loss_val = loss.item()\n",
    "            epoch_loss.append(loss_val)\n",
    "            if runner is not None:\n",
    "                runner.log({\"eval/loss\": loss_val})\n",
    "    return torch.mean(torch.tensor(epoch_loss)).item()\n",
    "\n",
    "def eval_model(dataloader, model, scoring_functions):\n",
    "    outputs = []\n",
    "    ys = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y in tqdm(dataloader):\n",
    "            outputs.append(model(x))\n",
    "            ys.append(y)\n",
    "\n",
    "    y_pred = torch.concat(outputs).detach().numpy()\n",
    "    ys = torch.concat(ys).float()\n",
    "    res = {}\n",
    "    for sf in scoring_functions:\n",
    "        name = sf.__name__\n",
    "        res[name] = sf(y_pred > 0.5, ys.float(), average=\"macro\")\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33me12325298\u001b[0m (\u001b[33me12325298-tu-wien\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tibor/Documents/msc-datascience/2024w/TUW-NLP2024/notebooks/wandb/run-20241225_214250-4uxytouv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert/runs/4uxytouv' target=\"_blank\">flowing-pond-9</a></strong> to <a href='https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert' target=\"_blank\">https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert/runs/4uxytouv' target=\"_blank\">https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert/runs/4uxytouv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:39<00:00,  7.96s/it]\n",
      "100%|██████████| 10/10 [00:19<00:00,  1.92s/it]\n",
      "100%|██████████| 20/20 [02:43<00:00,  8.20s/it]\n",
      "100%|██████████| 10/10 [00:19<00:00,  1.95s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▂▃▄▇▂▆▃▂▂▁▃▄▅█▂▇▄▃▂▂</td></tr><tr><td>train/loss</td><td>█▇▆▆▅▅▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.75951</td></tr><tr><td>train/loss</td><td>1.61688</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">flowing-pond-9</strong> at: <a href='https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert/runs/4uxytouv' target=\"_blank\">https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert/runs/4uxytouv</a><br> View project at: <a href='https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert' target=\"_blank\">https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241225_214250-4uxytouv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  1.92s/it]\n",
      "/home/tibor/Documents/msc-datascience/2024w/TUW-NLP2024/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/tibor/Documents/msc-datascience/2024w/TUW-NLP2024/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.09826925532132913,\n",
       " 'recall_score': 0.06090909090909092,\n",
       " 'precision_score': 0.5454545454545454}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "scoring_functions = [f1_score, recall_score, precision_score]\n",
    "\n",
    "\n",
    "lr = 0.0001\n",
    "batch_size = 5\n",
    "epochs = 2\n",
    "\n",
    "runner = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"tuw-nlp2024-multilanguage-bert\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"distiled-bert\",\n",
    "    \"epochs\": epochs\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "model = CustomBertClassifier(256, 22)\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "loss_fn = loss_fn = torch.nn.BCELoss()\n",
    "train_dl = DataLoader(train, batch_size=batch_size)\n",
    "eval_dl = DataLoader(val, batch_size=batch_size)\n",
    "test_dl = DataLoader(test, batch_size=batch_size)\n",
    "\n",
    "best_loss = torch.inf\n",
    "for i in range(epochs):\n",
    "    training_step(train_dl, model, optimizer, loss_fn, freeze_bert=True, runner=runner)\n",
    "    eval_loss = eval_step(eval_dl, model, loss_fn, runner=runner)\n",
    "\n",
    "runner.finish()\n",
    "eval_model(test_dl, model, scoring_functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:00, 4901.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language.EN LabelLevel.NARATIVES (200, 23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115it [00:00, 1959.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language.HI LabelLevel.NARATIVES (115, 23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:00, 1692.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language.PT LabelLevel.NARATIVES (200, 23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [00:00, 3306.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language.BG LabelLevel.NARATIVES (211, 23)\n",
      "(726, 23)\n"
     ]
    }
   ],
   "source": [
    "dataset = PropagandaDataset(\"../data\", languages=[Language.EN, Language.HI, Language.PT, Language.BG], tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased'))\n",
    "train, val, test = torch.utils.data.random_split(dataset, [400, 163, 163])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tibor/Documents/msc-datascience/2024w/TUW-NLP2024/notebooks/wandb/run-20241225_215909-2ldospcv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert/runs/2ldospcv' target=\"_blank\">spring-cloud-10</a></strong> to <a href='https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert' target=\"_blank\">https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert/runs/2ldospcv' target=\"_blank\">https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert/runs/2ldospcv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [10:11<00:00,  7.65s/it]\n",
      "100%|██████████| 33/33 [00:58<00:00,  1.78s/it]\n",
      "100%|██████████| 80/80 [10:03<00:00,  7.55s/it]\n",
      "100%|██████████| 33/33 [00:58<00:00,  1.79s/it]\n",
      "100%|██████████| 80/80 [10:35<00:00,  7.95s/it]\n",
      "100%|██████████| 33/33 [01:05<00:00,  1.98s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▄▃▂▅▅▆▆▅▃▂▃▅▃▂▆▅▆▃▆█▇▃▃▅▃▅▃▂▃▁▄▅▅▆▆▅▆▃▃▄</td></tr><tr><td>train/loss</td><td>██▆▆▅▅▅▄▄▅▃▃▃▄▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.78916</td></tr><tr><td>train/loss</td><td>1.24011</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spring-cloud-10</strong> at: <a href='https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert/runs/2ldospcv' target=\"_blank\">https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert/runs/2ldospcv</a><br> View project at: <a href='https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert' target=\"_blank\">https://wandb.ai/e12325298-tu-wien/tuw-nlp2024-multilanguage-bert</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241225_215909-2ldospcv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:59<00:00,  1.81s/it]\n",
      "/home/tibor/Documents/msc-datascience/2024w/TUW-NLP2024/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.09960433906754468,\n",
       " 'recall_score': 0.05688789737869493,\n",
       " 'precision_score': 0.5909090909090909}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "batch_size = 5\n",
    "epochs = 3\n",
    "\n",
    "runner = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"tuw-nlp2024-multilanguage-bert\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"distiled-bert\",\n",
    "    \"epochs\": epochs\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "model = CustomBertClassifier(256, 22)\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "loss_fn = loss_fn = torch.nn.BCELoss()\n",
    "train_dl = DataLoader(train, batch_size=batch_size)\n",
    "eval_dl = DataLoader(val, batch_size=batch_size)\n",
    "test_dl = DataLoader(test, batch_size=batch_size)\n",
    "\n",
    "best_loss = torch.inf\n",
    "for i in range(epochs):\n",
    "    training_step(train_dl, model, optimizer, loss_fn, freeze_bert=True, runner=runner)\n",
    "    eval_loss = eval_step(eval_dl, model, loss_fn, runner=runner)\n",
    "\n",
    "runner.finish()\n",
    "eval_model(test_dl, model, scoring_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:00, 5003.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language.EN LabelLevel.NARATIVES (200, 23)\n",
      "(200, 23)\n"
     ]
    }
   ],
   "source": [
    "dataset = PropagandaDataset(\"../data\", languages=[Language.EN], tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased'))\n",
    "train, val, test = torch.utils.data.random_split(dataset, [100, 50, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  1.95s/it]\n",
      "/home/tibor/Documents/msc-datascience/2024w/TUW-NLP2024/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/tibor/Documents/msc-datascience/2024w/TUW-NLP2024/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/tibor/Documents/msc-datascience/2024w/TUW-NLP2024/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.09748743842629337,\n",
       " 'recall_score': 0.059090909090909104,\n",
       " 'precision_score': 0.5909090909090909}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dl = DataLoader(test, batch_size=batch_size)\n",
    "eval_model(test_dl, model, scoring_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
