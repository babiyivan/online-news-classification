{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\IvanB\\\\AppData\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\tuw-nlp2024-HQEixHnN-py3.12\\\\Scripts'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "os.path.dirname(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IvanB\\OneDrive - TU Wien\\University\\TU Wien\\2024W\\194.093 Natural Language Processing and Information Extraction\\Ex\\TUW-NLP2024\n"
     ]
    }
   ],
   "source": [
    "# Define the new path\n",
    "new_path = r'C:\\Users\\IvanB\\OneDrive - TU Wien\\University\\TU Wien\\2024W\\194.093 Natural Language Processing and Information Extraction\\Ex\\TUW-NLP2024'\n",
    "os.chdir(new_path)\n",
    "# Verify the change\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tuwnlp.utils import Language, TokenType, LabelLevel\n",
    "from tuwnlp.utils import read_file_as_string, read_files_to_df\n",
    "from tuwnlp.utils import get_low_lvl_label_mappings, get_top_lvl_label_mappings\n",
    "from tuwnlp.utils import get_file_labels_dataframe\n",
    "\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IvanB\\OneDrive - TU Wien\\University\\TU Wien\\2024W\\194.093 Natural Language Processing and Information Extraction\\Ex\\TUW-NLP2024\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# Define the new path\n",
    "new_path = r'C:\\Users\\IvanB\\OneDrive - TU Wien\\University\\TU Wien\\2024W\\194.093 Natural Language Processing and Information Extraction\\Ex\\TUW-NLP2024\\notebooks'\n",
    "os.chdir(new_path)\n",
    "# Verify the change\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:00, 324.89it/s]\n"
     ]
    }
   ],
   "source": [
    "en_naratives_labels = get_file_labels_dataframe(\n",
    "    Path(\"../data\"),\n",
    "    Language.EN,\n",
    "    LabelLevel.NARATIVES\n",
    ")\n",
    "\n",
    "en_texts = read_files_to_df(Path(\"../data/tmp\"), Language.EN)\n",
    "en_texts.index = en_texts[\"file name\"].values\n",
    "en_texts = en_texts.drop(columns = [\"file name\", \"file path\"])\n",
    "df = pd.merge(en_texts, en_naratives_labels, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IvanB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tuw-nlp2024-HQEixHnN-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\IvanB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tuw-nlp2024-HQEixHnN-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6464538043169837, 0.6893424036281179, 0.7158793543104183)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "#Split into features and target\n",
    "text_col = \"text\"\n",
    "y_cols = [col for col in df.columns if col != text_col]\n",
    "X = df[text_col]\n",
    "y = df[y_cols]\n",
    "\n",
    "#build bag of words\n",
    "embedding_size = 1024\n",
    "vectorizer = CountVectorizer(max_features=embedding_size)\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "#build model\n",
    "model = MultiOutputClassifier(MultinomialNB())\n",
    "model.fit(X,y)\n",
    "\n",
    "#evaluate model\n",
    "y_pred = model.predict(X)\n",
    "macro = f1_score(y, y_pred, average=\"macro\")\n",
    "micro = f1_score(y, y_pred, average=\"micro\")\n",
    "weighted = f1_score(y, y_pred, average=\"weighted\")\n",
    "macro, micro, weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "EN_CC_100000    1\\tpentagon\\tPROPN\\tpentagon 2\\tplans\\tNOUN\\tp...\n",
      "EN_CC_100002    1\\toxford\\tPROPN\\toxford 2\\tresidents\\tPROPN\\t...\n",
      "EN_CC_100003    1\\tfonda\\tPROPN\\tfonda 2\\theads\\tVERB\\thead 3\\...\n",
      "EN_CC_100004    1\\ttesla\\tPROPN\\ttesla 2\\towner\\tNOUN\\towner 3...\n",
      "EN_CC_100005    1\\tclimate\\tNOUN\\tclimate 2\\tcrazies\\tNOUN\\tcr...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X.dtypes)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "200it [00:00, 3153.05it/s]\n",
      "C:\\Users\\IvanB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tuw-nlp2024-HQEixHnN-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\IvanB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tuw-nlp2024-HQEixHnN-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "1it [00:00,  3.80it/s]\n",
      "200it [00:00, 4028.41it/s]\n",
      "C:\\Users\\IvanB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tuw-nlp2024-HQEixHnN-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\IvanB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tuw-nlp2024-HQEixHnN-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2it [00:03,  1.85s/it]\n",
      "200it [00:00, 4237.40it/s]\n",
      "C:\\Users\\IvanB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tuw-nlp2024-HQEixHnN-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\IvanB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tuw-nlp2024-HQEixHnN-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "3it [00:03,  1.15s/it]\n",
      "200it [00:00, 3781.38it/s]\n",
      "C:\\Users\\IvanB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tuw-nlp2024-HQEixHnN-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\IvanB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tuw-nlp2024-HQEixHnN-py3.12\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "4it [00:14,  3.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import clone\n",
    "from tqdm import tqdm\n",
    "# LANGUAGES = [Language.BG, Language.EN, Language.HI, Language.PT]\n",
    "LANGUAGES = [Language.EN]\n",
    "LEVELS = [LabelLevel.NARATIVES, LabelLevel.SUBNARATIVES]\n",
    "TEXT_COL = \"text\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "models = [\n",
    "    MultiOutputClassifier(MultinomialNB()),\n",
    "    #MultiOutputClassifier(SVC()),\n",
    "    MultiOutputClassifier(RandomForestClassifier())\n",
    "]\n",
    "\n",
    "coombinations = product(LANGUAGES, LEVELS, models)\n",
    "\n",
    "all_res = []\n",
    "\n",
    "for language, level, model in tqdm(coombinations):\n",
    "    labels = get_file_labels_dataframe(\n",
    "    Path(\"../data\"),\n",
    "    language,\n",
    "    level,\n",
    ")\n",
    "    # Read and split into train and test subsets\n",
    "    text = read_files_to_df(Path(\"../data/tmp\"), language)\n",
    "    text.index = text[\"file name\"].values\n",
    "    text = text.drop(columns = [\"file name\", \"file path\"])\n",
    "    df = pd.merge(text, labels, left_index=True, right_index=True)\n",
    "    X = df[TEXT_COL]\n",
    "    y = df.drop(columns=[TEXT_COL])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RANDOM_STATE)\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_features=embedding_size)\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    res = {}\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    res[\"language\"] = language.value\n",
    "    res[\"level\"] = level.value\n",
    "    res[\"model\"] = clone(model)\n",
    "    res[\"macro\"] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    res[\"micro\"] = f1_score(y_test, y_pred, average=\"micro\")\n",
    "    res[\"weighted\"] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    all_res.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>level</th>\n",
       "      <th>model</th>\n",
       "      <th>macro</th>\n",
       "      <th>micro</th>\n",
       "      <th>weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN</td>\n",
       "      <td>NARATIVES</td>\n",
       "      <td>MultiOutputClassifier(estimator=MultinomialNB())</td>\n",
       "      <td>0.241246</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.492275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN</td>\n",
       "      <td>NARATIVES</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>0.042311</td>\n",
       "      <td>0.365591</td>\n",
       "      <td>0.287614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language      level                                              model  \\\n",
       "0       EN  NARATIVES   MultiOutputClassifier(estimator=MultinomialNB())   \n",
       "1       EN  NARATIVES  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "\n",
       "      macro     micro  weighted  \n",
       "0  0.241246  0.444444  0.492275  \n",
       "1  0.042311  0.365591  0.287614  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_res)\n",
    "df.query(\"level == 'NARATIVES'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform the test features to get back the original text\n",
    "X_test_text = vectorizer.inverse_transform(X_test)\n",
    "\n",
    "# Loop through and print misclassified examples\n",
    "misclassified_samples = []\n",
    "for i in range(X_test.shape[0]):  # Use X_test.shape[0] to get the number of samples\n",
    "    # Check if any label in the row is misclassified\n",
    "    if (y_test.iloc[i] != y_pred[i]).any():\n",
    "        # Get the misclassified example and append it to the list\n",
    "        misclassified_samples.append({\n",
    "            \"text\": \" \".join(X_test_text[i]),  # Reconstruct the text from the sparse vector\n",
    "            \"true_labels\": y_test.iloc[i].to_dict(),  # Convert true labels to dict\n",
    "            \"pred_labels\": y_pred[i].tolist()  # Convert predicted labels to list\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '10 100 101 102 103 104 105 106 107 108 109 11 110 111 112 113 114 115 116 117 118 119 12 120 121 122 123 124 125 126 127 128 129 13 130 131 132 133 134 135 136 137 138 139 14 140 141 142 143 144 145 146 147 148 149 15 150 151 152 153 154 155 156 157 158 159 16 160 161 162 163 164 165 166 167 168 169 17 170 171 172 173 174 175 176 177 178 179 18 180 181 182 183 184 185 186 187 188 189 19 190 191 192 193 194 195 196 197 198 199 20 200 201 202 203 204 205 206 207 208 209 21 210 211 212 213 214 215 216 217 218 219 22 220 221 222 223 224 225 226 227 228 229 23 230 231 232 233 234 235 236 237 238 239 24 240 241 242 243 244 245 246 247 248 249 25 250 251 252 253 254 255 256 257 258 259 26 260 261 262 263 264 265 266 267 268 269 27 270 271 272 273 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 accord according act action adj administration adp adv along also american around ask attack aux biden bomb call cause claim claimed claims committee continue council countries country damage economy entire environment facebook follow foreign france full gas germany get give hold infrastructure international invasion joe june last late later latest military ministry month months moscow must nations nato new news nord not noun nt num operation order part pipeline plant president pron propn putin putins read region report reported require response responsible russia russian said say sea security seek september state stream three top twitter ukraine un united us use using verb vladimir want war we week west year',\n",
       " 'true_labels': {'CC: Amplifying Climate Fears: Amplifying existing fears of global warming': False,\n",
       "  'CC: Amplifying Climate Fears: Doomsday scenarios for humans': False,\n",
       "  'CC: Amplifying Climate Fears: Earth will be uninhabitable soon': False,\n",
       "  'CC: Amplifying Climate Fears: Other': False,\n",
       "  'CC: Amplifying Climate Fears: Whatever we do it is already too late': False,\n",
       "  'CC: Climate change is beneficial: CO2 is beneficial': False,\n",
       "  'CC: Climate change is beneficial: Other': False,\n",
       "  'CC: Climate change is beneficial: Temperature increase is beneficial': False,\n",
       "  'CC: Controversy about green technologies: Nuclear energy is not climate friendly': False,\n",
       "  'CC: Controversy about green technologies: Other': False,\n",
       "  'CC: Controversy about green technologies: Renewable energy is costly': False,\n",
       "  'CC: Controversy about green technologies: Renewable energy is dangerous': False,\n",
       "  'CC: Controversy about green technologies: Renewable energy is unreliable': False,\n",
       "  'CC: Criticism of climate movement: Ad hominem attacks on key activists': False,\n",
       "  'CC: Criticism of climate movement: Climate movement is alarmist': False,\n",
       "  'CC: Criticism of climate movement: Climate movement is corrupt': False,\n",
       "  'CC: Criticism of climate movement: Other': False,\n",
       "  'CC: Criticism of climate policies: Climate policies are ineffective': False,\n",
       "  'CC: Criticism of climate policies: Climate policies are only for profit': False,\n",
       "  'CC: Criticism of climate policies: Climate policies have negative impact on the economy': False,\n",
       "  'CC: Criticism of climate policies: Other': False,\n",
       "  'CC: Criticism of institutions and authorities: Criticism of international entities': False,\n",
       "  'CC: Criticism of institutions and authorities: Criticism of national governments': False,\n",
       "  'CC: Criticism of institutions and authorities: Criticism of political organizations and figures': False,\n",
       "  'CC: Criticism of institutions and authorities: Criticism of the EU': False,\n",
       "  'CC: Criticism of institutions and authorities: Other': False,\n",
       "  'CC: Downplaying climate change: CO2 concentrations are too small to have an impact': False,\n",
       "  'CC: Downplaying climate change: Climate cycles are natural': False,\n",
       "  'CC: Downplaying climate change: Human activities do not impact climate change': False,\n",
       "  'CC: Downplaying climate change: Humans and nature will adapt to the changes': False,\n",
       "  'CC: Downplaying climate change: Ice is not melting': False,\n",
       "  'CC: Downplaying climate change: Other': False,\n",
       "  'CC: Downplaying climate change: Sea levels are not rising': False,\n",
       "  'CC: Downplaying climate change: Temperature increase does not have significant impact': False,\n",
       "  'CC: Downplaying climate change: Weather suggests the trend is global cooling': False,\n",
       "  'CC: Green policies are geopolitical instruments: Climate-related international relations are abusive/exploitative': False,\n",
       "  'CC: Green policies are geopolitical instruments: Green activities are a form of neo-colonialism': False,\n",
       "  'CC: Green policies are geopolitical instruments: Other': False,\n",
       "  'CC: Hidden plots by secret schemes of powerful groups: Blaming global elites': False,\n",
       "  'CC: Hidden plots by secret schemes of powerful groups: Climate agenda has hidden motives': False,\n",
       "  'CC: Hidden plots by secret schemes of powerful groups: Other': False,\n",
       "  'CC: Questioning the measurements and science: Data shows no temperature increase': False,\n",
       "  'CC: Questioning the measurements and science: Greenhouse effect/carbon dioxide do not drive climate change': False,\n",
       "  'CC: Questioning the measurements and science: Methodologies/metrics used are unreliable/faulty': False,\n",
       "  'CC: Questioning the measurements and science: Other': False,\n",
       "  'CC: Questioning the measurements and science: Scientific community is unreliable': False,\n",
       "  'Other': False,\n",
       "  'URW: Amplifying war-related fears: By continuing the war we risk WWIII': False,\n",
       "  'URW: Amplifying war-related fears: NATO should/will directly intervene': False,\n",
       "  'URW: Amplifying war-related fears: Other': False,\n",
       "  'URW: Amplifying war-related fears: Russia will also attack other countries': False,\n",
       "  'URW: Amplifying war-related fears: There is a real possibility that nuclear weapons will be employed': False,\n",
       "  'URW: Blaming the war on others rather than the invader: Other': False,\n",
       "  'URW: Blaming the war on others rather than the invader: The West are the aggressors': True,\n",
       "  'URW: Blaming the war on others rather than the invader: Ukraine is the aggressor': False,\n",
       "  'URW: Discrediting Ukraine: Discrediting Ukrainian government and officials and policies': False,\n",
       "  'URW: Discrediting Ukraine: Discrediting Ukrainian military': False,\n",
       "  'URW: Discrediting Ukraine: Discrediting Ukrainian nation and society': False,\n",
       "  'URW: Discrediting Ukraine: Other': False,\n",
       "  'URW: Discrediting Ukraine: Rewriting Ukraineâ€™s history': False,\n",
       "  'URW: Discrediting Ukraine: Situation in Ukraine is hopeless': False,\n",
       "  'URW: Discrediting Ukraine: Ukraine is a hub for criminal activities': False,\n",
       "  'URW: Discrediting Ukraine: Ukraine is a puppet of the West': False,\n",
       "  'URW: Discrediting Ukraine: Ukraine is associated with nazism': False,\n",
       "  'URW: Discrediting the West, Diplomacy: Diplomacy does/will not work': False,\n",
       "  'URW: Discrediting the West, Diplomacy: Other': False,\n",
       "  'URW: Discrediting the West, Diplomacy: The EU is divided': False,\n",
       "  'URW: Discrediting the West, Diplomacy: The West does not care about Ukraine, only about its interests': False,\n",
       "  'URW: Discrediting the West, Diplomacy: The West is overreacting': False,\n",
       "  'URW: Discrediting the West, Diplomacy: The West is weak': False,\n",
       "  'URW: Discrediting the West, Diplomacy: West is tired of Ukraine': False,\n",
       "  'URW: Distrust towards Media: Other': False,\n",
       "  'URW: Distrust towards Media: Ukrainian media cannot be trusted': False,\n",
       "  'URW: Distrust towards Media: Western media is an instrument of propaganda': False,\n",
       "  'URW: Hidden plots by secret schemes of powerful groups: Other': False,\n",
       "  'URW: Negative Consequences for the West: Other': False,\n",
       "  'URW: Negative Consequences for the West: Sanctions imposed by Western countries will backfire': False,\n",
       "  'URW: Negative Consequences for the West: The conflict will increase the Ukrainian refugee flows to Europe': False,\n",
       "  'URW: Overpraising the West: NATO will destroy Russia': False,\n",
       "  'URW: Overpraising the West: Other': False,\n",
       "  'URW: Overpraising the West: The West belongs in the right side of history': False,\n",
       "  'URW: Overpraising the West: The West has the strongest international support': False,\n",
       "  'URW: Praise of Russia: Other': False,\n",
       "  'URW: Praise of Russia: Praise of Russian President Vladimir Putin': False,\n",
       "  'URW: Praise of Russia: Praise of Russian military might': False,\n",
       "  'URW: Praise of Russia: Russia has international support from a number of countries and people': False,\n",
       "  'URW: Praise of Russia: Russia is a guarantor of peace and prosperity': False,\n",
       "  'URW: Praise of Russia: Russian invasion has strong national support': False,\n",
       "  'URW: Russia is the Victim: Other': False,\n",
       "  'URW: Russia is the Victim: Russia actions in Ukraine are only self-defence': False,\n",
       "  'URW: Russia is the Victim: The West is russophobic': False,\n",
       "  'URW: Russia is the Victim: UA is anti-RU extremists': False,\n",
       "  'URW: Speculating war outcomes: Other': False,\n",
       "  'URW: Speculating war outcomes: Russian army is collapsing': False,\n",
       "  'URW: Speculating war outcomes: Russian army will lose all the occupied territories': False,\n",
       "  'URW: Speculating war outcomes: Ukrainian army is collapsing': False},\n",
       " 'pred_labels': [False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample0 = misclassified_samples[0]  # Accessing sample 0\n",
    "# print(f\"Text: {sample0['text']}\\n\")\n",
    "# print(f\"True Labels: {sample0['true_labels']}\\n\")\n",
    "# print(f\"Predicted Labels: {sample0['pred_labels']}\\n\")\n",
    "sample0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch found for label: \"URW: Blaming the war on others rather than the invader: The West are the aggressors\"\n",
      "True label: True\n",
      "Predicted label: False\n",
      "\n",
      "Number of mismatches for sample0: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare true labels with predicted labels and count mismatches\n",
    "# For sample0\n",
    "mismatches0 = 0\n",
    "for idx, (label, true_value) in enumerate(sample0['true_labels'].items()):\n",
    "    predicted_value = sample0['pred_labels'][idx]\n",
    "    if true_value != predicted_value:\n",
    "        mismatches0 += 1\n",
    "        print(f'Mismatch found for label: \"{label}\"')\n",
    "        print(f'True label: {true_value}')\n",
    "        print(f'Predicted label: {predicted_value}\\n')\n",
    "print(f'Number of mismatches for sample0: {mismatches0}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = misclassified_samples[1]  # Accessing second sample 1\n",
    "# print(f\"Text: {sample1['text']}\\n\")\n",
    "# print(f\"True Labels: {sample1['true_labels']}\\n\")\n",
    "# print(f\"Predicted Labels: {sample1['pred_labels']}\\n\")\n",
    "#sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch found for label: \"CC: Criticism of climate movement: Ad hominem attacks on key activists\"\n",
      "True label: False\n",
      "Predicted label: True\n",
      "\n",
      "Mismatch found for label: \"CC: Downplaying climate change: Climate cycles are natural\"\n",
      "True label: False\n",
      "Predicted label: True\n",
      "\n",
      "Mismatch found for label: \"CC: Downplaying climate change: Ice is not melting\"\n",
      "True label: False\n",
      "Predicted label: True\n",
      "\n",
      "Number of mismatches for sample1: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare true labels with predicted labels and count mismatches\n",
    "# For sample1\n",
    "mismatches1 = 0\n",
    "for idx, (label, true_value) in enumerate(sample1['true_labels'].items()):\n",
    "    predicted_value = sample1['pred_labels'][idx]\n",
    "    if true_value != predicted_value:\n",
    "        mismatches1 += 1\n",
    "        print(f'Mismatch found for label: \"{label}\"')\n",
    "        print(f'True label: {true_value}')\n",
    "        print(f'Predicted label: {predicted_value}\\n')\n",
    "print(f'Number of mismatches for sample1: {mismatches1}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = misclassified_samples[2]  # Accessing second sample 1\n",
    "# print(f\"Text: {sample2['text']}\\n\")\n",
    "# print(f\"True Labels: {sample2['true_labels']}\\n\")\n",
    "# print(f\"Predicted Labels: {sample2['pred_labels']}\\n\")\n",
    "#sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch found for label: \"Other\"\n",
      "True label: True\n",
      "Predicted label: False\n",
      "\n",
      "Number of mismatches for sample2: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare true labels with predicted labels and count mismatches\n",
    "# For sample2\n",
    "mismatches2 = 0\n",
    "for idx, (label, true_value) in enumerate(sample2['true_labels'].items()):\n",
    "    predicted_value = sample2['pred_labels'][idx]\n",
    "    if true_value != predicted_value:\n",
    "        mismatches2 += 1\n",
    "        print(f'Mismatch found for label: \"{label}\"')\n",
    "        print(f'True label: {true_value}')\n",
    "        print(f'Predicted label: {predicted_value}\\n')\n",
    "print(f'Number of mismatches for sample2: {mismatches2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample3 = misclassified_samples[3]  # Accessing second sample 1\n",
    "# print(f\"Text: {sample3['text']}\\n\")\n",
    "# print(f\"True Labels: {sample3['true_labels']}\\n\")\n",
    "# print(f\"Predicted Labels: {sample3['pred_labels']}\\n\")\n",
    "#sample3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch found for label: \"CC: Criticism of climate policies: Other\"\n",
      "True label: True\n",
      "Predicted label: False\n",
      "\n",
      "Mismatch found for label: \"CC: Criticism of institutions and authorities: Criticism of political organizations and figures\"\n",
      "True label: True\n",
      "Predicted label: False\n",
      "\n",
      "Number of mismatches for sample3: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare true labels with predicted labels and count mismatches\n",
    "# For sample3\n",
    "mismatches3 = 0\n",
    "for idx, (label, true_value) in enumerate(sample3['true_labels'].items()):\n",
    "    predicted_value = sample3['pred_labels'][idx]\n",
    "    if true_value != predicted_value:\n",
    "        mismatches3 += 1\n",
    "        print(f'Mismatch found for label: \"{label}\"')\n",
    "        print(f'True label: {true_value}')\n",
    "        print(f'Predicted label: {predicted_value}\\n')\n",
    "print(f'Number of mismatches for sample3: {mismatches3}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample4 = misclassified_samples[4]  # Accessing second sample 1\n",
    "# print(f\"Text: {sample4['text']}\\n\")\n",
    "# print(f\"True Labels: {sample4['true_labels']}\\n\")\n",
    "# print(f\"Predicted Labels: {sample4['pred_labels']}\\n\")\n",
    "#sample4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch found for label: \"URW: Amplifying war-related fears: Other\"\n",
      "True label: True\n",
      "Predicted label: False\n",
      "\n",
      "Mismatch found for label: \"URW: Amplifying war-related fears: Russia will also attack other countries\"\n",
      "True label: True\n",
      "Predicted label: False\n",
      "\n",
      "Mismatch found for label: \"URW: Amplifying war-related fears: There is a real possibility that nuclear weapons will be employed\"\n",
      "True label: True\n",
      "Predicted label: False\n",
      "\n",
      "Number of mismatches for sample4: 3\n"
     ]
    }
   ],
   "source": [
    "# Compare true labels with predicted labels and count mismatches\n",
    "mismatches4 = 0\n",
    "for idx, (label, true_value) in enumerate(sample4['true_labels'].items()):\n",
    "    predicted_value = sample4['pred_labels'][idx]\n",
    "    if true_value != sample4['pred_labels'][idx]:\n",
    "        mismatches4 += 1\n",
    "        print(f'Mismatch found for label: \"{label}\"')\n",
    "        print(f'True label: {true_value}')\n",
    "        print(f'Predicted label: {predicted_value}\\n')\n",
    "print(f'Number of mismatches for sample4: {mismatches4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on the Qualitative Analysis\n",
    "\n",
    "In reviewing the mismatches found across the 5 samples, several key observations and potential patterns were identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inconsistent classification for similar statements\n",
    "\n",
    "A recurring mismatch involves statements that discuss similar topics or sentiments but are classified differently by the model. For example, in sample1 \"CC: Criticism of climate movement: Ad hominem attacks on key activists\"** and \"CC: Downplaying climate change: Ice is not melting\" are classified as True in the ground truth, yet predicted as False. These errors may point to a misclassification in cases where there are nuanced debates around climate change policies or activism. These topics might overlap with broader political and social ideologies, which makes the classification task challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideological complexity\n",
    "Several labels in the mismatch list involve ideological positions that can be interpreted in multiple ways depending on one’s perspective. For example, i sample4 the statement \"URW: Amplifying war-related fears: The West will attack other countries\" is difficult because it involves predicting a future political scenario with implications for geopolitics. The label itself is a reflection of a specific ideological viewpoint (the belief that the West might provoke a larger conflict). The model may have trouble distinguishing between valid concerns and propagandist narratives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Other\" label\n",
    "In sample1 and sample3, many mismatches occur for statements labeled \"Other\". For example, the statement \"CC: Criticism of climate policies: Other\" is misclassified as False in sample1. The label \"Other\" generally indicates broad or less easily definable categories, or at least the label that applis in case none of the other labels in the same topic (in this case \"Criticism of climate policies\") apply. This could also point to a dataset challenge where labels within the “Other” category are either too vague or inconsistently applied, making it harder for the model to develop a reliable classification rule for such instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiguity\n",
    "The model seems to struggle when faced with complex or ambiguous phrasing. For instance, in sample2, the statement \"CC: Criticism of climate policies: Climate policies have negative impact on the economy\" was misclassified, indicating that the model may be challenged by nuanced criticisms that require a deeper understanding of the economic implications of climate policies.\n",
    "\n",
    "Similarly, sample4 contains multiple predictions around political and war-related fears (e.g., \"The West will attack other countries\" and \"The real possibility of nuclear weapons\"), where the model might be overlooking subtle differences in the phrasing or context in these complex political arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several key takeaways from the analysed mismatches (quantitative analysis) are:\n",
    "\n",
    "- More often than not, the model incorrectly predicts 'False' when the actual label is 'True', this seems to be a pattern across the sampled cases, the only exception being sample1.\n",
    "\n",
    "- The model may struggle because the statements involve subjective interpretations that depend on political ideology, which may cause it to misclassify them.\n",
    "\n",
    "- Labels such as \"Other\" and other generalized criticisms (e.g., \"Climate policies are ineffective\") seem to be frequently misclassified. The model might fail to grasp the broader context or interpret these vague categories correctly, leading to errors.\n",
    "\n",
    "- Statements that critique complex subjects like climate change policies or specific persons (e.g., \"Ad hominem attacks on key activists\") involve subtle language and multi-layered arguments, which might be difficult for the model to properly interpret, causing misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
