{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and Constants\n",
    "Core dependencies and configuration constants for the narrative classification system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/.pyenv/versions/3.12.7/envs/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import os\n",
    "import tqdm\n",
    "import zipfile\n",
    "from conllu import parse\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import hamming_loss, f1_score, classification_report, precision_recall_fscore_support\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "\n",
    "# Configuration constants\n",
    "TARGET_LANG = ['EN', 'PT', 'RU']\n",
    "RAW_DATASET_PATH = '../data/raw/target_4_December_release'\n",
    "PREPROCESSED_DATASET_PATH = '../data/preprocessed/preprocessed_target_4_December_release'\n",
    "LABELS_PATH = [os.path.join(RAW_DATASET_PATH, lang, 'subtask-2-annotations.txt') for lang in TARGET_LANG]\n",
    "INPUTS_PATH = [os.path.join(PREPROCESSED_DATASET_PATH, lang) for lang in TARGET_LANG]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Preprocessing\n",
    "Functions for loading and preprocessing the narrative classification dataset.\n",
    "Includes file extraction, label mapping, and text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datasets():\n",
    "    \"\"\"Extract datasets from zip files if not already extracted.\"\"\"\n",
    "    if not os.path.exists(RAW_DATASET_PATH):\n",
    "        with zipfile.ZipFile(RAW_DATASET_PATH + '.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall(RAW_DATASET_PATH, pwd=b'narratives5202trainTHREE')\n",
    "    \n",
    "    if not os.path.exists(PREPROCESSED_DATASET_PATH):\n",
    "        with zipfile.ZipFile(PREPROCESSED_DATASET_PATH + '.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall(PREPROCESSED_DATASET_PATH)\n",
    "\n",
    "def load_and_map_labels(label_file_paths):\n",
    "    \"\"\"Load and process narrative labels from files.\"\"\"\n",
    "    all_labels = []\n",
    "    all_narratives_set = set()\n",
    "    all_subnarratives_set = set()\n",
    "    \n",
    "    for label_file_path in label_file_paths:\n",
    "        labels_df = pd.read_csv(\n",
    "            label_file_path, \n",
    "            sep=\"\\t\", \n",
    "            header=None, \n",
    "            names=[\"article_id\", \"narratives\", \"subnarratives\"]\n",
    "        )\n",
    "        \n",
    "        for _, row in labels_df.iterrows():\n",
    "            narratives = row[\"narratives\"].split(\";\") if pd.notna(row[\"narratives\"]) else []\n",
    "            subnarratives = row[\"subnarratives\"].split(\";\") if pd.notna(row[\"subnarratives\"]) else []\n",
    "            \n",
    "            all_narratives_set.update(narratives)\n",
    "            all_subnarratives_set.update(subnarratives)\n",
    "            \n",
    "            all_labels.append({\n",
    "                \"article_id\": row[\"article_id\"],\n",
    "                \"narratives\": narratives,\n",
    "                \"subnarratives\": subnarratives\n",
    "            })\n",
    "    \n",
    "    # Remove empty strings and sort\n",
    "    all_narratives = sorted(list(all_narratives_set - {''} if '' in all_narratives_set else all_narratives_set))\n",
    "    all_subnarratives = sorted(list(all_subnarratives_set - {''} if '' in all_subnarratives_set else all_subnarratives_set))\n",
    "    \n",
    "    return pd.DataFrame(all_labels), all_narratives, all_subnarratives\n",
    "\n",
    "def parse_conllu_file(file_path):\n",
    "    \"\"\"Parse a CoNLL-U format file and concatenate tokens.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "    token_lists = parse(data)\n",
    "    return \" \".join(token[\"form\"] for token_list in token_lists for token in token_list)\n",
    "\n",
    "def map_input_to_label_with_lang(articles_paths, article_ids, labels):\n",
    "    \"\"\"Map articles to labels and add language information.\"\"\"\n",
    "    labels = labels.set_index(\"article_id\")\n",
    "    articles_data = []\n",
    "    \n",
    "    for articles_path in articles_paths:\n",
    "        lang = articles_path.split('/')[-1]\n",
    "        \n",
    "        for article_id in article_ids:\n",
    "            file_path = os.path.join(articles_path, f\"{article_id.replace('.txt', '.conllu')}\")\n",
    "            if os.path.exists(file_path) and article_id in labels.index:\n",
    "                article_text = parse_conllu_file(file_path)\n",
    "                article_labels = labels.loc[article_id]\n",
    "                articles_data.append({\n",
    "                    \"article_id\": article_id,\n",
    "                    \"text\": article_text,\n",
    "                    \"narratives\": article_labels[\"narratives\"],\n",
    "                    \"subnarratives\": article_labels[\"subnarratives\"],\n",
    "                    \"language\": lang\n",
    "                })\n",
    "    return pd.DataFrame(articles_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset and Model Components\n",
    "Core components for the narrative classification model including custom dataset,\n",
    "loss function, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NarrativeDataset(Dataset):\n",
    "    \"\"\"Custom dataset for narrative classification.\"\"\"\n",
    "    def __init__(self, articles, tokenizer, max_len, task_type='narrative'):\n",
    "        self.articles = articles\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.task_type = task_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = self.articles.iloc[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            article[\"text\"],\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = article[f\"{self.task_type}_labels\"]\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "class CustomBCEWithLogitsLoss(nn.Module):\n",
    "    \"\"\"Custom BCE loss with label-dependent weighting.\"\"\"\n",
    "    def __init__(self, pos_weight=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, logits, target):\n",
    "        batch_pos_counts = torch.sum(target, dim=0)\n",
    "        batch_neg_counts = target.size(0) - batch_pos_counts\n",
    "        \n",
    "        eps = 1e-7\n",
    "        batch_weights = (batch_neg_counts + eps) / (batch_pos_counts + eps)\n",
    "        \n",
    "        if self.pos_weight is not None:\n",
    "            batch_weights = batch_weights * self.pos_weight\n",
    "            \n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight=batch_weights, reduction=self.reduction)\n",
    "        return loss_fn(logits, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preparation and Analysis\n",
    "Functions for preparing data splits, analyzing label distributions,\n",
    "and computing class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label_data(df):\n",
    "    \"\"\"Prepare data with improved label handling and class weights.\"\"\"\n",
    "    mlb_narrative = MultiLabelBinarizer()\n",
    "    mlb_subnarrative = MultiLabelBinarizer()\n",
    "    \n",
    "    narrative_labels = mlb_narrative.fit_transform(df[\"narratives\"])\n",
    "    subnarrative_labels = mlb_subnarrative.fit_transform(df[\"subnarratives\"])\n",
    "    \n",
    "    narrative_classes = list(mlb_narrative.classes_)\n",
    "    subnarrative_classes = list(mlb_subnarrative.classes_)\n",
    "    \n",
    "    narrative_weights = compute_class_weights(narrative_labels)\n",
    "    subnarrative_weights = compute_class_weights(subnarrative_labels)\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[\"narrative_labels\"] = list(narrative_labels)\n",
    "    df[\"subnarrative_labels\"] = list(subnarrative_labels)\n",
    "    \n",
    "    return df, narrative_weights, subnarrative_weights, narrative_classes, subnarrative_classes\n",
    "\n",
    "def prepare_data_for_split(df, narrative_classes, subnarrative_classes):\n",
    "    \"\"\"\n",
    "    Prepare validation or test split using the classes from training data.\n",
    "    \"\"\"\n",
    "    # Encode narrative labels using training classes\n",
    "    narrative_labels = df[\"narratives\"].apply(\n",
    "        lambda x: [1 if n in x else 0 for n in narrative_classes]\n",
    "    ).tolist()\n",
    "    \n",
    "    # Encode subnarrative labels using training classes\n",
    "    subnarrative_labels = df[\"subnarratives\"].apply(\n",
    "        lambda x: [1 if sn in x else 0 for sn in subnarrative_classes]\n",
    "    ).tolist()\n",
    "    \n",
    "    # Add encoded labels to dataframe\n",
    "    df = df.copy()\n",
    "    df[\"narrative_labels\"] = narrative_labels\n",
    "    df[\"subnarrative_labels\"] = subnarrative_labels\n",
    "      \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_language_specific_splits(df, test_size=0.1, val_size=0.1, train_target_ratio=0.6, random_state=42, target_lang='EN'):\n",
    "    \"\"\"\n",
    "    Create train/val/test splits with controlled amount of target language in training.\n",
    "    \"\"\"\n",
    "    \n",
    "    target_lang_df = df[df['language'] == target_lang]\n",
    "    other_langs_df = df[df['language'] != target_lang]\n",
    "    \n",
    "    total_target = len(target_lang_df)\n",
    "    test_samples = int(total_target * test_size)\n",
    "    val_samples = int(total_target * val_size)\n",
    "    train_samples = int(total_target * train_target_ratio)\n",
    "    \n",
    "    remaining_target_df, test_df = train_test_split(\n",
    "        target_lang_df,\n",
    "        test_size=test_samples,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    train_target_df, val_df = train_test_split(\n",
    "        remaining_target_df,\n",
    "        test_size=val_samples,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # If we want more training samples, take them from what's left\n",
    "    if len(train_target_df) > train_samples:\n",
    "        train_target_df = train_target_df.sample(n=train_samples, random_state=random_state)\n",
    "    \n",
    "    # Combine target language training data with other languages\n",
    "    train_df = pd.concat([other_langs_df, train_target_df])\n",
    "    \n",
    "    train_df = train_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\nData split sizes:\")\n",
    "    print(f\"Total samples: {len(train_df)}\")\n",
    "    print(\"Language distribution:\")\n",
    "    print(train_df['language'].value_counts())\n",
    "    print(f\"Validation set ({target_lang} only):\")\n",
    "    print(f\"Total samples: {len(val_df)}\")\n",
    "    print(f\"Test set ({target_lang} only):\")\n",
    "    print(f\"Total samples: {len(test_df)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def compute_class_weights(labels):\n",
    "    \"\"\"Compute balanced class weights with clipping.\"\"\"\n",
    "    pos_counts = np.sum(labels, axis=0)\n",
    "    neg_counts = len(labels) - pos_counts\n",
    "    weights = neg_counts / (pos_counts + 1e-7)\n",
    "    return torch.FloatTensor(np.clip(weights, 0.1, 10.0))\n",
    "\n",
    "def create_weighted_sampler(labels):\n",
    "    \"\"\"Create weighted sampler for handling class imbalance.\"\"\"\n",
    "    label_counts = np.sum(labels, axis=0)\n",
    "    weights = 1.0 / label_counts\n",
    "    weights = np.nan_to_num(weights, nan=1.0, posinf=1.0)\n",
    "    sample_weights = np.sum(labels * weights, axis=1)\n",
    "    return WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "def analyze_rare_labels(df):\n",
    "    \"\"\"Analyze and report rare labels in the dataset.\"\"\"\n",
    "    narrative_counts = {}\n",
    "    subnarrative_counts = {}\n",
    "    \n",
    "    for narratives in df[\"narratives\"]:\n",
    "        for n in narratives:\n",
    "            narrative_counts[n] = narrative_counts.get(n, 0) + 1\n",
    "    \n",
    "    for subnarratives in df[\"subnarratives\"]:\n",
    "        for sn in subnarratives:\n",
    "            subnarrative_counts[sn] = subnarrative_counts.get(sn, 0) + 1\n",
    "    \n",
    "    rare_narratives = {k: v for k, v in narrative_counts.items() if v <= 2}\n",
    "    rare_subnarratives = {k: v for k, v in subnarrative_counts.items() if v <= 2}\n",
    "    \n",
    "    print(\"\\nRare Label Analysis:\")\n",
    "    print(f\"Total unique narratives: {len(narrative_counts)}\")\n",
    "    print(f\"Rare narratives (<=2 occurrences): {len(rare_narratives)}\")\n",
    "    print(f\"Total unique subnarratives: {len(subnarrative_counts)}\")\n",
    "    print(f\"Rare subnarratives (<=2 occurrences): {len(rare_subnarratives)}\")\n",
    "    \n",
    "    return rare_narratives, rare_subnarratives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Training and Evaluation\n",
    "Functions for model training, threshold optimization, and performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_improved(model, train_loader, val_loader, device, task_type, narrative_classes, subnarrative_classes, class_weights, num_epochs=5):\n",
    "    \"\"\"Train model with improved handling of multilabel classification.\"\"\"\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "    criterion = CustomBCEWithLogitsLoss(pos_weight=class_weights.to(device))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "    \n",
    "    best_f1 = 0\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    best_threshold = 0.5\n",
    "    thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm.tqdm(train_loader, desc=f\"Training {task_type} model - Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits, inputs['labels'])\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        best_threshold = optimize_threshold(model, val_loader, device, thresholds)\n",
    "        y_pred, y_true = get_predictions(model, val_loader, device, threshold=best_threshold)\n",
    "        class_labels = narrative_classes if task_type == 'narrative' else subnarrative_classes\n",
    "        val_metrics = evaluate_model(y_pred, y_true, class_labels, print_report=True)\n",
    "        \n",
    "        scheduler.step(val_metrics['Macro F1'])\n",
    "        \n",
    "        if val_metrics['Macro F1'] > best_f1:\n",
    "            best_f1 = val_metrics['Macro F1']\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'best_threshold': best_threshold,\n",
    "                'best_f1': best_f1\n",
    "            }, f'best_{task_type}_model.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch + 1}\")\n",
    "                break\n",
    "    \n",
    "    return model, best_threshold\n",
    "\n",
    "def get_predictions(model, data_loader, device, threshold=0.3):\n",
    "    \"\"\"Generate predictions from the model.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > threshold).int()\n",
    "            \n",
    "            all_predictions.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    return torch.cat(all_predictions, dim=0).numpy(), torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "def evaluate_model(y_pred, y_true, class_labels, print_report=False):\n",
    "    \"\"\"Evaluate model performance using multiple metrics.\"\"\"\n",
    "    metrics = {\n",
    "        \"Hamming Loss\": hamming_loss(y_true, y_pred),\n",
    "        \"Macro F1\": f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        \"Micro F1\": f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "        \"Subset Accuracy\": (y_true == y_pred).all(axis=1).mean()\n",
    "    }\n",
    "    \n",
    "    if print_report:\n",
    "        active_classes = np.where(y_true.sum(axis=0) > 0)[0]\n",
    "        active_labels = [class_labels[i] for i in active_classes]\n",
    "        \n",
    "        y_true_filtered = y_true[:, active_classes]\n",
    "        y_pred_filtered = y_pred[:, active_classes]\n",
    "        \n",
    "        print(\"\\nClassification Report (Active Classes Only):\")\n",
    "        print(classification_report(\n",
    "            y_true_filtered,\n",
    "            y_pred_filtered,\n",
    "            target_names=active_labels,\n",
    "            digits=2,\n",
    "            zero_division=0\n",
    "        ))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def optimize_threshold(model, val_loader, device, thresholds):\n",
    "    \"\"\"Find optimal classification threshold.\"\"\"\n",
    "    model.eval()\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            all_logits.append(outputs.logits)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    logits = torch.cat(all_logits, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        preds = (probs > threshold).float()\n",
    "        f1 = f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "def evaluate_global_performance(narrative_model, subnarrative_model, val_narrative_loader, val_subnarrative_loader, device, narrative_threshold=0.3, subnarrative_threshold=0.3):\n",
    "    \"\"\"Evaluate both models together and compute global metrics.\"\"\"\n",
    "    narrative_preds, narrative_true = get_predictions(\n",
    "        narrative_model, val_narrative_loader, device, narrative_threshold)\n",
    "    subnarrative_preds, subnarrative_true = get_predictions(\n",
    "        subnarrative_model, val_subnarrative_loader, device, subnarrative_threshold)\n",
    "    \n",
    "    return compute_global_score(\n",
    "        narrative_preds, narrative_true,\n",
    "        subnarrative_preds, subnarrative_true\n",
    "    )\n",
    "\n",
    "def compute_global_score(narrative_preds, narrative_true, subnarrative_preds, subnarrative_true, narrative_weight=0.4, subnarrative_weight=0.6):\n",
    "    \"\"\"\n",
    "    Compute weighted global performance metrics including precision and recall.\n",
    "    Returns detailed metrics for both narrative and subnarrative levels.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_metrics(preds, true):\n",
    "        # Calculate precision, recall, f1-score for both macro and micro averages\n",
    "        macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "            true, preds, average='macro', zero_division=0\n",
    "        )\n",
    "        micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
    "            true, preds, average='micro', zero_division=0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"hamming\": hamming_loss(true, preds),\n",
    "            \"macro_precision\": macro_precision,\n",
    "            \"macro_recall\": macro_recall,\n",
    "            \"macro_f1\": macro_f1,\n",
    "            \"micro_precision\": micro_precision,\n",
    "            \"micro_recall\": micro_recall,\n",
    "            \"micro_f1\": micro_f1,\n",
    "            \"subset_acc\": (true == preds).all(axis=1).mean()\n",
    "        }\n",
    "    \n",
    "    narrative_metrics = compute_metrics(narrative_preds, narrative_true)\n",
    "    subnarrative_metrics = compute_metrics(subnarrative_preds, subnarrative_true)\n",
    "    \n",
    "    # Calculate weighted global metrics\n",
    "    global_metrics = {}\n",
    "    for metric in narrative_metrics.keys():\n",
    "        global_metrics[f\"global_{metric}\"] = (\n",
    "            narrative_weight * narrative_metrics[metric] +\n",
    "            subnarrative_weight * subnarrative_metrics[metric]\n",
    "        )\n",
    "    \n",
    "    hierarchical_accuracy = (\n",
    "        (narrative_true == narrative_preds).all(axis=1) &\n",
    "        (subnarrative_true == subnarrative_preds).all(axis=1)\n",
    "    ).mean()\n",
    "    \n",
    "    return {\n",
    "        \"narrative_metrics\": narrative_metrics,\n",
    "        \"subnarrative_metrics\": subnarrative_metrics,\n",
    "        \"global_metrics\": global_metrics,\n",
    "        \"hierarchical_accuracy\": hierarchical_accuracy\n",
    "    }\n",
    "\n",
    "def print_all_metrics(metrics_dict):\n",
    "    \"\"\"Print all metrics in a formatted way\"\"\"\n",
    "    for level, metrics in metrics_dict.items():\n",
    "        if level != \"hierarchical_accuracy\":\n",
    "            print(f\"\\n{level.replace('_', ' ').title()}:\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"{metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nHierarchical Accuracy: {metrics_dict['hierarchical_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Main Execution Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rare Label Analysis:\n",
      "Total unique narratives: 22\n",
      "Rare narratives (<=2 occurrences): 0\n",
      "Total unique subnarratives: 92\n",
      "Rare subnarratives (<=2 occurrences): 12\n",
      "\n",
      "Data split sizes:\n",
      "Total samples: 683\n",
      "Language distribution:\n",
      "language\n",
      "PT    384\n",
      "EN    166\n",
      "RU    133\n",
      "Name: count, dtype: int64\n",
      "Validation set (EN only):\n",
      "Total samples: 27\n",
      "Test set (EN only):\n",
      "Total samples: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m\n\u001b[1;32m     31\u001b[0m test_data \u001b[38;5;241m=\u001b[39m prepare_data_for_split(test_df, narrative_classes, subnarrative_classes)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Initialize models\u001b[39;00m\n\u001b[1;32m     34\u001b[0m narrative_model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-multilingual-cased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnarrative_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmulti_label_classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m---> 38\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m subnarrative_model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-multilingual-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     42\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(subnarrative_classes),\n\u001b[1;32m     43\u001b[0m     problem_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     46\u001b[0m narrative_test_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/envs/nlp/lib/python3.12/site-packages/transformers/modeling_utils.py:3110\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3106\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3109\u001b[0m         )\n\u001b[0;32m-> 3110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Extract and load data\n",
    "extract_datasets()\n",
    "labels_df, all_narratives, all_subnarratives = load_and_map_labels(LABELS_PATH)\n",
    "\n",
    "# Process and prepare data\n",
    "article_ids = labels_df[\"article_id\"]\n",
    "df = map_input_to_label_with_lang(INPUTS_PATH, article_ids, labels_df)\n",
    "\n",
    "# Remove excess \"Other\" labels\n",
    "other_df = df[\n",
    "    df[\"narratives\"].apply(lambda x: any(\"Other\" in item for item in x)) & \n",
    "    df[\"subnarratives\"].apply(lambda x: any(\"Other\" in item for item in x))\n",
    "].sample(frac=0.7, random_state=42)\n",
    "df = df.drop(other_df.index)\n",
    "\n",
    "# Analyze labels and create splits\n",
    "rare_narratives, rare_subnarratives = analyze_rare_labels(df)\n",
    "train_df, val_df, test_df = create_language_specific_splits(\n",
    "    df, test_size=0.1, val_size=0.1, train_target_ratio=0.6,\n",
    "    random_state=42, target_lang='EN'\n",
    ")\n",
    "\n",
    "# Prepare data and create datasets\n",
    "train_data, narrative_weights, subnarrative_weights, narrative_classes, subnarrative_classes = process_label_data(\n",
    "    train_df)\n",
    "val_data = prepare_data_for_split(val_df, narrative_classes, subnarrative_classes)\n",
    "test_data = prepare_data_for_split(test_df, narrative_classes, subnarrative_classes)\n",
    "\n",
    "# Initialize models\n",
    "narrative_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=len(narrative_classes),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ").to(device)\n",
    "\n",
    "subnarrative_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=len(subnarrative_classes),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ").to(device)\n",
    "\n",
    "narrative_test_loader = None\n",
    "subnarrative_test_loader = None\n",
    "narrative_threshold = None\n",
    "subnarrative_threshold = None\n",
    "\n",
    "# Create data loaders and train models\n",
    "for task_type, model, weights, classes in [\n",
    "    ('narrative', narrative_model, narrative_weights, narrative_classes),\n",
    "    ('subnarrative', subnarrative_model, subnarrative_weights, subnarrative_classes)\n",
    "]:\n",
    "    print(f\"\\nTraining {task_type} model...\")\n",
    "    train_dataset = NarrativeDataset(train_data, tokenizer, max_len=512, task_type=task_type)\n",
    "    val_dataset = NarrativeDataset(val_data, tokenizer, max_len=512, task_type=task_type)\n",
    "    test_dataset = NarrativeDataset(test_data, tokenizer, max_len=512, task_type=task_type)\n",
    "    \n",
    "    sampler = create_weighted_sampler(train_data[f\"{task_type}_labels\"].tolist())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    if task_type == 'narrative':\n",
    "        narrative_test_loader = test_loader\n",
    "    else:\n",
    "        subnarrative_test_loader = test_loader\n",
    "    \n",
    "    model, threshold = train_model_improved(\n",
    "        model, train_loader, val_loader, device,\n",
    "        task_type, narrative_classes, subnarrative_classes,\n",
    "        weights, num_epochs=10\n",
    "    )\n",
    "    \n",
    "    if task_type == 'narrative':\n",
    "        narrative_threshold = threshold\n",
    "    else:\n",
    "        subnarrative_threshold = threshold\n",
    "\n",
    "print(\"\\nGenerating final predictions and computing metrics...\")\n",
    "narrative_preds, narrative_true = get_predictions(\n",
    "    narrative_model, narrative_test_loader, device, narrative_threshold)\n",
    "subnarrative_preds, subnarrative_true = get_predictions(\n",
    "    subnarrative_model, subnarrative_test_loader, device, subnarrative_threshold)\n",
    "\n",
    "scores = compute_global_score(\n",
    "    narrative_preds, narrative_true,\n",
    "    subnarrative_preds, subnarrative_true\n",
    ")\n",
    "\n",
    "print_all_metrics(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
