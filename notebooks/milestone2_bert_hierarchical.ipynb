{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import os\n",
    "import tqdm\n",
    "import zipfile\n",
    "from conllu import parse\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import hamming_loss, f1_score, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep existing constants and data loading code...\n",
    "TARGET_LANG = ['EN', 'PT', 'RU']\n",
    "RAW_DATASET_PATH = '../data/raw/target_4_December_release'\n",
    "PREPROCESSED_DATASET_PATH = '../data/preprocessed/preprocessed_target_4_December_release'\n",
    "LABELS_PATH = [os.path.join(RAW_DATASET_PATH, lang, 'subtask-2-annotations.txt') for lang in TARGET_LANG]\n",
    "INPUTS_PATH = [os.path.join(PREPROCESSED_DATASET_PATH, lang) for lang in TARGET_LANG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract datasets if needed\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "def extract_datasets():\n",
    "    if not os.path.exists(RAW_DATASET_PATH):\n",
    "        with zipfile.ZipFile(RAW_DATASET_PATH + '.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall(RAW_DATASET_PATH, pwd=b'narratives5202trainTHREE')\n",
    "    \n",
    "    if not os.path.exists(PREPROCESSED_DATASET_PATH):\n",
    "        with zipfile.ZipFile(PREPROCESSED_DATASET_PATH + '.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall(PREPROCESSED_DATASET_PATH)\n",
    "\n",
    "def load_and_map_labels(label_file_paths: list[str]):\n",
    "    \"\"\"Load and map narrative labels from files.\"\"\"\n",
    "    all_labels = []\n",
    "    all_narratives_set = set()\n",
    "    all_subnarratives_set = set()\n",
    "    \n",
    "    for label_file_path in label_file_paths:\n",
    "        labels_df = pd.read_csv(\n",
    "            label_file_path, \n",
    "            sep=\"\\t\", \n",
    "            header=None, \n",
    "            names=[\"article_id\", \"narratives\", \"subnarratives\"]\n",
    "        )\n",
    "        \n",
    "        for _, row in labels_df.iterrows():\n",
    "            # Extract narratives and subnarratives\n",
    "            narratives = row[\"narratives\"].split(\";\") if pd.notna(row[\"narratives\"]) else []\n",
    "            subnarratives = row[\"subnarratives\"].split(\";\") if pd.notna(row[\"subnarratives\"]) else []\n",
    "            \n",
    "            # Update sets of unique labels\n",
    "            all_narratives_set.update(narratives)\n",
    "            all_subnarratives_set.update(subnarratives)\n",
    "            \n",
    "            all_labels.append({\n",
    "                \"article_id\": row[\"article_id\"],\n",
    "                \"narratives\": narratives,\n",
    "                \"subnarratives\": subnarratives\n",
    "            })\n",
    "    \n",
    "    # Convert sets to sorted lists for consistent ordering\n",
    "    all_narratives = sorted(list(all_narratives_set - {''} if '' in all_narratives_set else all_narratives_set))\n",
    "    all_subnarratives = sorted(list(all_subnarratives_set - {''} if '' in all_subnarratives_set else all_subnarratives_set))\n",
    "    \n",
    "    return pd.DataFrame(all_labels), all_narratives, all_subnarratives\n",
    "\n",
    "def parse_conllu_file(file_path):\n",
    "    \"\"\"Parse a CoNLL-U format file and return concatenated tokens.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "    token_lists = parse(data)\n",
    "    all_tokens = [token[\"form\"] for token_list in token_lists for token in token_list]\n",
    "    return \" \".join(all_tokens)\n",
    "\n",
    "def map_input_to_label(articles_paths: list[str], article_ids: list[str], labels: pd.DataFrame):\n",
    "    \"\"\"Map input articles to their corresponding labels.\"\"\"\n",
    "    labels = labels.set_index(\"article_id\")\n",
    "    \n",
    "    articles_data = []\n",
    "    for articles_path in articles_paths:\n",
    "        for article_id in article_ids:\n",
    "            file_path = os.path.join(articles_path, f\"{article_id.replace('.txt', '.conllu')}\")\n",
    "            if os.path.exists(file_path) and article_id in labels.index:\n",
    "                article_text = parse_conllu_file(file_path)\n",
    "                article_labels = labels.loc[article_id]\n",
    "                articles_data.append({\n",
    "                    \"article_id\": article_id,\n",
    "                    \"text\": article_text,\n",
    "                    \"narratives\": article_labels[\"narratives\"],\n",
    "                    \"subnarratives\": article_labels[\"subnarratives\"]\n",
    "                })\n",
    "    return pd.DataFrame(articles_data)\n",
    "\n",
    "class NarrativeDataset(Dataset):\n",
    "    def __init__(self, articles, tokenizer, max_len, task_type='narrative'):\n",
    "        self.articles = articles\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.task_type = task_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = self.articles.iloc[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            article[\"text\"],\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        if self.task_type == 'narrative':\n",
    "            labels = article[\"narrative_labels\"]\n",
    "        else:\n",
    "            labels = article[\"subnarrative_labels\"]\n",
    "            \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "def get_predictions(model, data_loader, device, threshold=0.3):\n",
    "    \"\"\"Generate predictions from the model.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > threshold).int()\n",
    "            \n",
    "            all_predictions.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    return torch.cat(all_predictions, dim=0).numpy(), torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "def evaluate_model(y_pred, y_true, class_labels, print_report=False):\n",
    "    \"\"\"Evaluate model performance using multiple metrics.\"\"\"\n",
    "    hamming = hamming_loss(y_true, y_pred)\n",
    "    \n",
    "    # Handle zero division in F1 calculation\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    \n",
    "    subset_accuracy = (y_true == y_pred).all(axis=1).mean()\n",
    "    \n",
    "    if print_report:\n",
    "        # Only use actual classes that appear in the data\n",
    "        active_classes = np.where(y_true.sum(axis=0) > 0)[0]\n",
    "        active_labels = [class_labels[i] for i in active_classes]\n",
    "        \n",
    "        # Filter predictions and true values to only include active classes\n",
    "        y_true_filtered = y_true[:, active_classes]\n",
    "        y_pred_filtered = y_pred[:, active_classes]\n",
    "        \n",
    "        report = classification_report(\n",
    "            y_true_filtered, \n",
    "            y_pred_filtered,\n",
    "            target_names=active_labels,\n",
    "            digits=2,\n",
    "            zero_division=0\n",
    "        )\n",
    "        print(\"\\nClassification Report (Active Classes Only):\\n\")\n",
    "        print(report)\n",
    "    \n",
    "    return {\n",
    "        \"Hamming Loss\": hamming,\n",
    "        \"Macro F1\": macro_f1,\n",
    "        \"Micro F1\": micro_f1,\n",
    "        \"Subset Accuracy\": subset_accuracy\n",
    "    }\n",
    "\n",
    "def create_weighted_sampler(labels):\n",
    "    \"\"\"Create a weighted sampler to handle class imbalance.\"\"\"\n",
    "    label_counts = np.sum(labels, axis=0)\n",
    "    weights = 1.0 / label_counts\n",
    "    weights = np.nan_to_num(weights, nan=1.0, posinf=1.0)\n",
    "    sample_weights = np.sum(labels * weights, axis=1)\n",
    "    return WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "def prepare_data(df, all_narratives, all_subnarratives):\n",
    "    \"\"\"Prepare separate datasets for narratives and subnarratives.\"\"\"\n",
    "    narrative_labels = df[\"narratives\"].apply(\n",
    "        lambda x: [1 if n in x else 0 for n in all_narratives]\n",
    "    ).tolist()\n",
    "    \n",
    "    subnarrative_labels = df[\"subnarratives\"].apply(\n",
    "        lambda x: [1 if sn in x else 0 for sn in all_subnarratives]\n",
    "    ).tolist()\n",
    "    \n",
    "    df[\"narrative_labels\"] = narrative_labels\n",
    "    df[\"subnarrative_labels\"] = subnarrative_labels\n",
    "    \n",
    "    return df\n",
    "\n",
    "class CustomBCEWithLogitsLoss(nn.Module):\n",
    "    \"\"\"Custom loss function with label-dependent weighting.\"\"\"\n",
    "    def __init__(self, pos_weight=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, logits, target):\n",
    "        # Calculate label frequencies in this batch\n",
    "        batch_pos_counts = torch.sum(target, dim=0)\n",
    "        batch_neg_counts = target.size(0) - batch_pos_counts\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        eps = 1e-7\n",
    "        batch_weights = (batch_neg_counts + eps) / (batch_pos_counts + eps)\n",
    "        \n",
    "        if self.pos_weight is not None:\n",
    "            batch_weights = batch_weights * self.pos_weight\n",
    "            \n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight=batch_weights, reduction=self.reduction)\n",
    "        return loss_fn(logits, target)\n",
    "\n",
    "def analyze_label_distribution(df, task_type='subnarrative'):\n",
    "    \"\"\"Analyze and print label distribution statistics.\"\"\"\n",
    "    labels = df[f\"{task_type}_labels\"].tolist()\n",
    "    label_sums = np.sum(labels, axis=0)\n",
    "    \n",
    "    print(f\"\\n{task_type.capitalize()} Label Distribution:\")\n",
    "    print(f\"Total samples: {len(labels)}\")\n",
    "    print(f\"Average labels per sample: {np.mean(np.sum(labels, axis=1)):.2f}\")\n",
    "    print(f\"Label cardinality: {np.mean(label_sums):.2f}\")\n",
    "    print(f\"Label density: {np.mean(label_sums)/len(labels):.4f}\")\n",
    "    \n",
    "    # Calculate and print label correlations\n",
    "    label_matrix = np.array(labels)\n",
    "    correlations = np.corrcoef(label_matrix.T)\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(correlations)):\n",
    "        for j in range(i+1, len(correlations)):\n",
    "            if abs(correlations[i,j]) > 0.5:  # Threshold for high correlation\n",
    "                high_corr_pairs.append((i, j, correlations[i,j]))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(\"\\nHighly correlated label pairs:\")\n",
    "        for i, j, corr in high_corr_pairs[:5]:  # Show top 5\n",
    "            print(f\"Labels {i} and {j}: {corr:.2f}\")\n",
    "    \n",
    "    return label_sums\n",
    "\n",
    "def prepare_improved_data(df, all_narratives, all_subnarratives):\n",
    "    \"\"\"Prepare data with improved label handling.\"\"\"\n",
    "    # Create binary label matrices\n",
    "    mlb_narrative = MultiLabelBinarizer()\n",
    "    mlb_subnarrative = MultiLabelBinarizer()\n",
    "    \n",
    "    narrative_labels = mlb_narrative.fit_transform(df[\"narratives\"])\n",
    "    subnarrative_labels = mlb_subnarrative.fit_transform(df[\"subnarratives\"])\n",
    "    \n",
    "    # Store the label classes\n",
    "    narrative_classes = list(mlb_narrative.classes_)\n",
    "    subnarrative_classes = list(mlb_subnarrative.classes_)\n",
    "    \n",
    "    print(f\"Number of narrative classes: {len(narrative_classes)}\")\n",
    "    print(f\"Number of subnarrative classes: {len(subnarrative_classes)}\")\n",
    "    \n",
    "    # Calculate class weights\n",
    "    narrative_weights = compute_class_weights(narrative_labels)\n",
    "    subnarrative_weights = compute_class_weights(subnarrative_labels)\n",
    "    \n",
    "    # Add labels to dataframe\n",
    "    df[\"narrative_labels\"] = list(narrative_labels)\n",
    "    df[\"subnarrative_labels\"] = list(subnarrative_labels)\n",
    "    \n",
    "    return (df, narrative_weights, subnarrative_weights, \n",
    "            narrative_classes, subnarrative_classes)\n",
    "\n",
    "def compute_class_weights(labels):\n",
    "    \"\"\"Compute balanced class weights.\"\"\"\n",
    "    pos_counts = np.sum(labels, axis=0)\n",
    "    neg_counts = len(labels) - pos_counts\n",
    "    \n",
    "    # Balanced weight calculation\n",
    "    weights = neg_counts / (pos_counts + 1e-7)\n",
    "    weights = np.clip(weights, 0.1, 10.0)  # Clip weights to prevent extreme values\n",
    "    \n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "def create_stratified_splits(df, test_size=0.1, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Create train/val/test splits handling rare labels appropriately\n",
    "    \"\"\"\n",
    "    # Create simplified stratification labels for more robust splitting\n",
    "    n_labels = df[\"narratives\"].apply(len)\n",
    "    sn_labels = df[\"subnarratives\"].apply(len)\n",
    "    \n",
    "    # Bin the label counts to create more stable groups\n",
    "    def bin_counts(x):\n",
    "        if x == 0:\n",
    "            return '0'\n",
    "        elif x == 1:\n",
    "            return '1'\n",
    "        elif x <= 3:\n",
    "            return '2-3'\n",
    "        else:\n",
    "            return '4+'\n",
    "    \n",
    "    stratify_labels = pd.DataFrame({\n",
    "        'n_count': n_labels.apply(bin_counts),\n",
    "        'sn_count': sn_labels.apply(bin_counts)\n",
    "    }).apply(lambda x: f\"{x['n_count']}_{x['sn_count']}\", axis=1)\n",
    "    \n",
    "    print(\"\\nStratification group sizes:\")\n",
    "    print(stratify_labels.value_counts())\n",
    "    \n",
    "    # Remove stratification if any group is too small\n",
    "    if stratify_labels.value_counts().min() < 3:\n",
    "        print(\"\\nWarning: Some stratification groups are too small.\")\n",
    "        print(\"Performing random split instead of stratified split.\")\n",
    "        stratify_labels = None\n",
    "    \n",
    "    # First split to get test set\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=stratify_labels if stratify_labels is not None else None\n",
    "    )\n",
    "    \n",
    "    # For the second split, create new stratification labels if needed\n",
    "    if stratify_labels is not None:\n",
    "        stratify_labels_trainval = pd.DataFrame({\n",
    "            'n_count': train_val_df[\"narratives\"].apply(len).apply(bin_counts),\n",
    "            'sn_count': train_val_df[\"subnarratives\"].apply(len).apply(bin_counts)\n",
    "        }).apply(lambda x: f\"{x['n_count']}_{x['sn_count']}\", axis=1)\n",
    "        \n",
    "        if stratify_labels_trainval.value_counts().min() < 3:\n",
    "            stratify_labels_trainval = None\n",
    "    else:\n",
    "        stratify_labels_trainval = None\n",
    "    \n",
    "    # Calculate validation size relative to train+val size\n",
    "    effective_val_size = val_size / (1 - test_size)\n",
    "    \n",
    "    # Second split to get train and validation sets\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size=effective_val_size,\n",
    "        random_state=random_state,\n",
    "        stratify=stratify_labels_trainval\n",
    "    )\n",
    "    \n",
    "    # Print split sizes and analyze distributions\n",
    "    print(\"\\nData split sizes:\")\n",
    "    print(f\"Training set: {len(train_df)} samples ({len(train_df)/len(df):.1%})\")\n",
    "    print(f\"Validation set: {len(val_df)} samples ({len(val_df)/len(df):.1%})\")\n",
    "    print(f\"Test set: {len(test_df)} samples ({len(test_df)/len(df):.1%})\")\n",
    "    \n",
    "    def analyze_label_distribution(data, split_name):\n",
    "        n_dist = data[\"narratives\"].apply(len).value_counts().sort_index()\n",
    "        sn_dist = data[\"subnarratives\"].apply(len).value_counts().sort_index()\n",
    "        \n",
    "        # Calculate rare label statistics\n",
    "        rare_narratives = set()\n",
    "        rare_subnarratives = set()\n",
    "        \n",
    "        for narratives in data[\"narratives\"]:\n",
    "            for n in narratives:\n",
    "                count = sum(1 for x in data[\"narratives\"] if n in x)\n",
    "                if count <= 2:\n",
    "                    rare_narratives.add(n)\n",
    "        \n",
    "        for subnarratives in data[\"subnarratives\"]:\n",
    "            for sn in subnarratives:\n",
    "                count = sum(1 for x in data[\"subnarratives\"] if sn in x)\n",
    "                if count <= 2:\n",
    "                    rare_subnarratives.add(sn)\n",
    "        \n",
    "        print(f\"\\n{split_name} set statistics:\")\n",
    "        print(f\"Number of samples: {len(data)}\")\n",
    "        print(f\"Labels per sample (narratives): {dict(n_dist)}\")\n",
    "        print(f\"Labels per sample (subnarratives): {dict(sn_dist)}\")\n",
    "        print(f\"Rare narratives (<=2 occurrences): {len(rare_narratives)}\")\n",
    "        print(f\"Rare subnarratives (<=2 occurrences): {len(rare_subnarratives)}\")\n",
    "    \n",
    "    analyze_label_distribution(train_df, \"Training\")\n",
    "    analyze_label_distribution(val_df, \"Validation\")\n",
    "    analyze_label_distribution(test_df, \"Test\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def analyze_rare_labels(df):\n",
    "    \"\"\"Analyze and print information about rare labels in the dataset\"\"\"\n",
    "    narrative_counts = {}\n",
    "    subnarrative_counts = {}\n",
    "    \n",
    "    # Count occurrences of each label\n",
    "    for narratives in df[\"narratives\"]:\n",
    "        for n in narratives:\n",
    "            narrative_counts[n] = narrative_counts.get(n, 0) + 1\n",
    "    \n",
    "    for subnarratives in df[\"subnarratives\"]:\n",
    "        for sn in subnarratives:\n",
    "            subnarrative_counts[sn] = subnarrative_counts.get(sn, 0) + 1\n",
    "    \n",
    "    # Find rare labels\n",
    "    rare_narratives = {k: v for k, v in narrative_counts.items() if v <= 2}\n",
    "    rare_subnarratives = {k: v for k, v in subnarrative_counts.items() if v <= 2}\n",
    "    \n",
    "    print(\"\\nRare Label Analysis:\")\n",
    "    print(f\"Total unique narratives: {len(narrative_counts)}\")\n",
    "    print(f\"Rare narratives (<=2 occurrences): {len(rare_narratives)}\")\n",
    "    print(\"Rare narrative counts:\", rare_narratives)\n",
    "    \n",
    "    print(f\"\\nTotal unique subnarratives: {len(subnarrative_counts)}\")\n",
    "    print(f\"Rare subnarratives (<=2 occurrences): {len(rare_subnarratives)}\")\n",
    "    print(\"Rare subnarrative counts:\", rare_subnarratives)\n",
    "    \n",
    "    return rare_narratives, rare_subnarratives\n",
    "\n",
    "def optimize_threshold(model, val_loader, device, thresholds):\n",
    "    \"\"\"Find the optimal threshold for classification.\"\"\"\n",
    "    model.eval()\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            all_logits.append(outputs.logits)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    logits = torch.cat(all_logits, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        preds = (probs > threshold).float()\n",
    "        f1 = f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "def prepare_data_for_split(df, narrative_classes, subnarrative_classes):\n",
    "    \"\"\"\n",
    "    Prepare validation or test split using the classes from training data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the split data\n",
    "        narrative_classes: List of narrative classes from training data\n",
    "        subnarrative_classes: List of subnarrative classes from training data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with encoded labels matching training data structure\n",
    "    \"\"\"\n",
    "    # Encode narrative labels using training classes\n",
    "    narrative_labels = df[\"narratives\"].apply(\n",
    "        lambda x: [1 if n in x else 0 for n in narrative_classes]\n",
    "    ).tolist()\n",
    "    \n",
    "    # Encode subnarrative labels using training classes\n",
    "    subnarrative_labels = df[\"subnarratives\"].apply(\n",
    "        lambda x: [1 if sn in x else 0 for sn in subnarrative_classes]\n",
    "    ).tolist()\n",
    "    \n",
    "    # Add encoded labels to dataframe\n",
    "    df = df.copy()\n",
    "    df[\"narrative_labels\"] = narrative_labels\n",
    "    df[\"subnarrative_labels\"] = subnarrative_labels\n",
    "      \n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_global_score(narrative_preds, narrative_true, subnarrative_preds, subnarrative_true, \n",
    "                        narrative_weight=0.4, subnarrative_weight=0.6):\n",
    "    \"\"\"\n",
    "    Compute a weighted global score combining narrative and subnarrative predictions.\n",
    "    \n",
    "    Args:\n",
    "        narrative_preds: Predictions for narratives\n",
    "        narrative_true: True labels for narratives\n",
    "        subnarrative_preds: Predictions for subnarratives\n",
    "        subnarrative_true: True labels for subnarratives\n",
    "        narrative_weight: Weight for narrative metrics (default: 0.4)\n",
    "        subnarrative_weight: Weight for subnarrative metrics (default: 0.6)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Combined metrics and individual metrics for both levels\n",
    "    \"\"\"\n",
    "    # Compute metrics for narratives\n",
    "    narrative_metrics = {\n",
    "        \"hamming\": hamming_loss(narrative_true, narrative_preds),\n",
    "        \"macro_f1\": f1_score(narrative_true, narrative_preds, average='macro', zero_division=0),\n",
    "        \"micro_f1\": f1_score(narrative_true, narrative_preds, average='micro', zero_division=0),\n",
    "        \"subset_acc\": (narrative_true == narrative_preds).all(axis=1).mean()\n",
    "    }\n",
    "    \n",
    "    # Compute metrics for subnarratives\n",
    "    subnarrative_metrics = {\n",
    "        \"hamming\": hamming_loss(subnarrative_true, subnarrative_preds),\n",
    "        \"macro_f1\": f1_score(subnarrative_true, subnarrative_preds, average='macro', zero_division=0),\n",
    "        \"micro_f1\": f1_score(subnarrative_true, subnarrative_preds, average='micro', zero_division=0),\n",
    "        \"subset_acc\": (subnarrative_true == subnarrative_preds).all(axis=1).mean()\n",
    "    }\n",
    "    \n",
    "    # Compute global metrics\n",
    "    global_metrics = {\n",
    "        \"global_hamming\": (\n",
    "            narrative_weight * narrative_metrics[\"hamming\"] +\n",
    "            subnarrative_weight * subnarrative_metrics[\"hamming\"]\n",
    "        ),\n",
    "        \"global_macro_f1\": (\n",
    "            narrative_weight * narrative_metrics[\"macro_f1\"] +\n",
    "            subnarrative_weight * subnarrative_metrics[\"macro_f1\"]\n",
    "        ),\n",
    "        \"global_micro_f1\": (\n",
    "            narrative_weight * narrative_metrics[\"micro_f1\"] +\n",
    "            subnarrative_weight * subnarrative_metrics[\"micro_f1\"]\n",
    "        ),\n",
    "        \"global_subset_acc\": (\n",
    "            narrative_weight * narrative_metrics[\"subset_acc\"] +\n",
    "            subnarrative_weight * subnarrative_metrics[\"subset_acc\"]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Compute hierarchical accuracy (both levels must be correct)\n",
    "    hierarchical_accuracy = (\n",
    "        (narrative_true == narrative_preds).all(axis=1) &\n",
    "        (subnarrative_true == subnarrative_preds).all(axis=1)\n",
    "    ).mean()\n",
    "    \n",
    "    # Combine all metrics\n",
    "    all_metrics = {\n",
    "        \"narrative_metrics\": narrative_metrics,\n",
    "        \"subnarrative_metrics\": subnarrative_metrics,\n",
    "        \"global_metrics\": global_metrics,\n",
    "        \"hierarchical_accuracy\": hierarchical_accuracy\n",
    "    }\n",
    "    \n",
    "    return all_metrics\n",
    "\n",
    "def evaluate_global_performance(narrative_model, subnarrative_model, val_narrative_loader, val_subnarrative_loader, device, \n",
    "                                narrative_threshold=0.3, subnarrative_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Evaluate both models together on the validation set.\n",
    "    \"\"\"\n",
    "    # Get predictions for both levels\n",
    "    narrative_preds, narrative_true = get_predictions(\n",
    "        narrative_model, val_narrative_loader, device, narrative_threshold)\n",
    "    subnarrative_preds, subnarrative_true = get_predictions(\n",
    "        subnarrative_model, val_subnarrative_loader, device, subnarrative_threshold)\n",
    "    \n",
    "    # Compute global scores\n",
    "    global_scores = compute_global_score(\n",
    "        narrative_preds, narrative_true,\n",
    "        subnarrative_preds, subnarrative_true\n",
    "    )\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nGlobal Performance Metrics:\")\n",
    "    print(\"\\nNarrative Level Metrics:\")\n",
    "    for metric, value in global_scores[\"narrative_metrics\"].items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nSubnarrative Level Metrics:\")\n",
    "    for metric, value in global_scores[\"subnarrative_metrics\"].items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nGlobal Combined Metrics:\")\n",
    "    for metric, value in global_scores[\"global_metrics\"].items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nHierarchical Accuracy: {global_scores['hierarchical_accuracy']:.4f}\")\n",
    "    \n",
    "    return global_scores\n",
    "\n",
    "def train_model_improved(model, train_loader, val_loader, device, task_type, \n",
    "                        narrative_classes, subnarrative_classes, class_weights, num_epochs=5):\n",
    "    \"\"\"Improved training function with better handling of multilabel classification.\"\"\"\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "    criterion = CustomBCEWithLogitsLoss(pos_weight=class_weights.to(device))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', \n",
    "                                                         factor=0.5, patience=2)\n",
    "    \n",
    "    best_f1 = 0\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    # Variables for threshold adjustment\n",
    "    thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm.tqdm(train_loader, desc=\"Training\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits, inputs['labels'])\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Find best threshold on validation set\n",
    "        best_threshold = optimize_threshold(model, val_loader, device, thresholds)\n",
    "        \n",
    "        y_pred, y_true = get_predictions(model, val_loader, device, threshold=best_threshold)\n",
    "        class_labels = narrative_classes if task_type == 'narrative' else subnarrative_classes\n",
    "        val_metrics = evaluate_model(y_pred, y_true, class_labels, print_report=True)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"Best threshold: {best_threshold:.2f}\")\n",
    "        for metric, value in val_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Update learning rate based on F1 score\n",
    "        scheduler.step(val_metrics['Macro F1'])\n",
    "        \n",
    "        current_f1 = val_metrics['Macro F1']\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'best_threshold': best_threshold,\n",
    "                'best_f1': best_f1\n",
    "            }, f'best_{task_type}_model.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch + 1}\")\n",
    "                break\n",
    "    \n",
    "    return model, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract datasets\n",
    "extract_datasets()\n",
    "\n",
    "# Load labels and get unique narratives and subnarratives\n",
    "labels_df, all_narratives, all_subnarratives = load_and_map_labels(LABELS_PATH)\n",
    "\n",
    "print(f\"Found {len(all_narratives)} unique narratives and {len(all_subnarratives)} unique subnarratives\")\n",
    "\n",
    "# Map inputs to labels\n",
    "article_ids = labels_df[\"article_id\"]\n",
    "df = map_input_to_label(INPUTS_PATH, article_ids, labels_df)\n",
    "\n",
    "# Remove excess \"Other\" labels\n",
    "other_df = df[\n",
    "    df[\"narratives\"].apply(lambda x: any(\"Other\" in item for item in x)) & \n",
    "    df[\"subnarratives\"].apply(lambda x: any(\"Other\" in item for item in x))\n",
    "].sample(frac=0.7, random_state=42)\n",
    "df = df.drop(other_df.index)\n",
    "\n",
    "# Prepare data with improved handling\n",
    "df, narrative_weights, subnarrative_weights, narrative_classes, subnarrative_classes = prepare_improved_data(\n",
    "    df, all_narratives, all_subnarratives)\n",
    "\n",
    "# Analyze rare labels before splitting\n",
    "print(\"Analyzing label distribution before splitting...\")\n",
    "rare_narratives, rare_subnarratives = analyze_rare_labels(df)\n",
    "\n",
    "# Create splits with modified strategy\n",
    "train_df, val_df, test_df = create_stratified_splits(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    val_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Prepare training data and get class information\n",
    "train_data, narrative_weights, subnarrative_weights, narrative_classes, subnarrative_classes = prepare_improved_data(\n",
    "    train_df, all_narratives, all_subnarratives)\n",
    "\n",
    "# Prepare validation and test data using training classes\n",
    "val_data = prepare_data_for_split(val_df, narrative_classes, subnarrative_classes)\n",
    "test_data = prepare_data_for_split(test_df, narrative_classes, subnarrative_classes)\n",
    "\n",
    "# Create datasets\n",
    "train_narrative_dataset = NarrativeDataset(train_data, tokenizer, max_len=512, task_type='narrative')\n",
    "val_narrative_dataset = NarrativeDataset(val_data, tokenizer, max_len=512, task_type='narrative')\n",
    "test_narrative_dataset = NarrativeDataset(test_data, tokenizer, max_len=512, task_type='narrative')\n",
    "\n",
    "train_subnarrative_dataset = NarrativeDataset(train_data, tokenizer, max_len=512, task_type='subnarrative')\n",
    "val_subnarrative_dataset = NarrativeDataset(val_data, tokenizer, max_len=512, task_type='subnarrative')\n",
    "test_subnarrative_dataset = NarrativeDataset(test_data, tokenizer, max_len=512, task_type='subnarrative')\n",
    "\n",
    "# Create weighted samplers for training\n",
    "narrative_sampler = create_weighted_sampler(train_data[\"narrative_labels\"].tolist())\n",
    "subnarrative_sampler = create_weighted_sampler(train_data[\"subnarrative_labels\"].tolist())\n",
    "\n",
    "# Create data loaders\n",
    "train_narrative_loader = DataLoader(\n",
    "    train_narrative_dataset, \n",
    "    batch_size=16,\n",
    "    sampler=narrative_sampler,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_narrative_loader = DataLoader(\n",
    "    val_narrative_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_narrative_loader = DataLoader(\n",
    "    test_narrative_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "train_subnarrative_loader = DataLoader(\n",
    "    train_subnarrative_dataset,\n",
    "    batch_size=16,\n",
    "    sampler=subnarrative_sampler,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_subnarrative_loader = DataLoader(\n",
    "    val_subnarrative_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_subnarrative_loader = DataLoader(\n",
    "    test_subnarrative_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models with correct number of labels\n",
    "# Initialize models with correct number of labels\n",
    "narrative_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=len(narrative_classes),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ").to(device)\n",
    "\n",
    "subnarrative_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=len(subnarrative_classes),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ").to(device)\n",
    "\n",
    "# Train models with correct class labels\n",
    "print(\"\\nTraining Narrative Model...\")\n",
    "narrative_model, narrative_threshold = train_model_improved(\n",
    "    narrative_model,\n",
    "    train_narrative_loader,\n",
    "    val_narrative_loader,\n",
    "    device,\n",
    "    'narrative',\n",
    "    narrative_classes,\n",
    "    subnarrative_classes,\n",
    "    narrative_weights,\n",
    "    num_epochs=5\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Subnarrative Model...\")\n",
    "subnarrative_model, subnarrative_threshold = train_model_improved(\n",
    "    subnarrative_model,\n",
    "    train_subnarrative_loader,\n",
    "    val_subnarrative_loader,\n",
    "    device,\n",
    "    'subnarrative',\n",
    "    narrative_classes,\n",
    "    subnarrative_classes,\n",
    "    subnarrative_weights,\n",
    "    num_epochs=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate global performance on validation set\n",
    "print(\"\\nEvaluating global performance on validation set...\")\n",
    "val_global_scores = evaluate_global_performance(\n",
    "    narrative_model, subnarrative_model,\n",
    "    val_narrative_loader, val_subnarrative_loader,\n",
    "    device\n",
    ")\n",
    "\n",
    "# Evaluate global performance on test set\n",
    "print(\"\\nEvaluating global performance on test set...\")\n",
    "test_global_scores = evaluate_global_performance(\n",
    "    narrative_model, subnarrative_model,\n",
    "    test_narrative_loader, test_subnarrative_loader,\n",
    "    device,\n",
    "    narrative_threshold=narrative_threshold,\n",
    "    subnarrative_threshold=subnarrative_threshold\n",
    ")\n",
    "\n",
    "\n",
    "# Save final results\n",
    "results = {\n",
    "    'validation_scores': val_global_scores,\n",
    "    'test_scores': test_global_scores,\n",
    "    'narrative_threshold': narrative_threshold,\n",
    "    'subnarrative_threshold': subnarrative_threshold,\n",
    "    'model_parameters': {\n",
    "        'narrative_labels': len(narrative_classes),\n",
    "        'subnarrative_labels': len(subnarrative_classes)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\nFinal Performance Summary:\")\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(f\"Global Macro F1: {val_global_scores['global_metrics']['global_macro_f1']:.4f}\")\n",
    "print(f\"Hierarchical Accuracy: {val_global_scores['hierarchical_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"Global Macro F1: {test_global_scores['global_metrics']['global_macro_f1']:.4f}\")\n",
    "print(f\"Hierarchical Accuracy: {test_global_scores['hierarchical_accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
