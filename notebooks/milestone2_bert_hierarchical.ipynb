{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import os\n",
    "import tqdm\n",
    "import zipfile\n",
    "from conllu import parse\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import hamming_loss, f1_score, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep existing constants and data loading code...\n",
    "TARGET_LANG = ['EN', 'PT', 'RU']\n",
    "RAW_DATASET_PATH = '../data/raw/target_4_December_release'\n",
    "PREPROCESSED_DATASET_PATH = '../data/preprocessed/preprocessed_target_4_December_release'\n",
    "LABELS_PATH = [os.path.join(RAW_DATASET_PATH, lang, 'subtask-2-annotations.txt') for lang in TARGET_LANG]\n",
    "INPUTS_PATH = [os.path.join(PREPROCESSED_DATASET_PATH, lang) for lang in TARGET_LANG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datasets():\n",
    "    if not os.path.exists(RAW_DATASET_PATH):\n",
    "        with zipfile.ZipFile(RAW_DATASET_PATH + '.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall(RAW_DATASET_PATH, pwd=b'narratives5202trainTHREE')\n",
    "    \n",
    "    if not os.path.exists(PREPROCESSED_DATASET_PATH):\n",
    "        with zipfile.ZipFile(PREPROCESSED_DATASET_PATH + '.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall(PREPROCESSED_DATASET_PATH)\n",
    "\n",
    "def load_and_map_labels(label_file_paths: list[str]):\n",
    "    \"\"\"Load and map narrative labels from files.\"\"\"\n",
    "    all_labels = []\n",
    "    all_narratives_set = set()\n",
    "    all_subnarratives_set = set()\n",
    "    \n",
    "    for label_file_path in label_file_paths:\n",
    "        labels_df = pd.read_csv(\n",
    "            label_file_path, \n",
    "            sep=\"\\t\", \n",
    "            header=None, \n",
    "            names=[\"article_id\", \"narratives\", \"subnarratives\"]\n",
    "        )\n",
    "        \n",
    "        for _, row in labels_df.iterrows():\n",
    "            # Extract narratives and subnarratives\n",
    "            narratives = row[\"narratives\"].split(\";\") if pd.notna(row[\"narratives\"]) else []\n",
    "            subnarratives = row[\"subnarratives\"].split(\";\") if pd.notna(row[\"subnarratives\"]) else []\n",
    "            \n",
    "            # Update sets of unique labels\n",
    "            all_narratives_set.update(narratives)\n",
    "            all_subnarratives_set.update(subnarratives)\n",
    "            \n",
    "            all_labels.append({\n",
    "                \"article_id\": row[\"article_id\"],\n",
    "                \"narratives\": narratives,\n",
    "                \"subnarratives\": subnarratives\n",
    "            })\n",
    "    \n",
    "    # Convert sets to sorted lists for consistent ordering\n",
    "    all_narratives = sorted(list(all_narratives_set - {''} if '' in all_narratives_set else all_narratives_set))\n",
    "    all_subnarratives = sorted(list(all_subnarratives_set - {''} if '' in all_subnarratives_set else all_subnarratives_set))\n",
    "    \n",
    "    return pd.DataFrame(all_labels), all_narratives, all_subnarratives\n",
    "\n",
    "def parse_conllu_file(file_path):\n",
    "    \"\"\"Parse a CoNLL-U format file and return concatenated tokens.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "    token_lists = parse(data)\n",
    "    all_tokens = [token[\"form\"] for token_list in token_lists for token in token_list]\n",
    "    return \" \".join(all_tokens)\n",
    "\n",
    "def map_input_to_label_with_lang(articles_paths: list[str], article_ids: list[str], labels: pd.DataFrame):\n",
    "    \"\"\"Map input articles to their corresponding labels and add language information.\"\"\"\n",
    "    labels = labels.set_index(\"article_id\")\n",
    "    \n",
    "    articles_data = []\n",
    "    for articles_path in articles_paths:\n",
    "        # Extract language from path\n",
    "        lang = articles_path.split('/')[-1]  # Gets the language code (EN, PT, RU)\n",
    "        \n",
    "        for article_id in article_ids:\n",
    "            file_path = os.path.join(articles_path, f\"{article_id.replace('.txt', '.conllu')}\")\n",
    "            if os.path.exists(file_path) and article_id in labels.index:\n",
    "                article_text = parse_conllu_file(file_path)\n",
    "                article_labels = labels.loc[article_id]\n",
    "                articles_data.append({\n",
    "                    \"article_id\": article_id,\n",
    "                    \"text\": article_text,\n",
    "                    \"narratives\": article_labels[\"narratives\"],\n",
    "                    \"subnarratives\": article_labels[\"subnarratives\"],\n",
    "                    \"language\": lang\n",
    "                })\n",
    "    return pd.DataFrame(articles_data)\n",
    "\n",
    "class NarrativeDataset(Dataset):\n",
    "    def __init__(self, articles, tokenizer, max_len, task_type='narrative'):\n",
    "        self.articles = articles\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.task_type = task_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = self.articles.iloc[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            article[\"text\"],\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        if self.task_type == 'narrative':\n",
    "            labels = article[\"narrative_labels\"]\n",
    "        else:\n",
    "            labels = article[\"subnarrative_labels\"]\n",
    "            \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "def get_predictions(model, data_loader, device, threshold=0.3):\n",
    "    \"\"\"Generate predictions from the model.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > threshold).int()\n",
    "            \n",
    "            all_predictions.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    return torch.cat(all_predictions, dim=0).numpy(), torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "def evaluate_model(y_pred, y_true, class_labels, print_report=False):\n",
    "    \"\"\"Evaluate model performance using multiple metrics.\"\"\"\n",
    "    hamming = hamming_loss(y_true, y_pred)\n",
    "    \n",
    "    # Handle zero division in F1 calculation\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    \n",
    "    subset_accuracy = (y_true == y_pred).all(axis=1).mean()\n",
    "    \n",
    "    if print_report:\n",
    "        # Only use actual classes that appear in the data\n",
    "        active_classes = np.where(y_true.sum(axis=0) > 0)[0]\n",
    "        active_labels = [class_labels[i] for i in active_classes]\n",
    "        \n",
    "        # Filter predictions and true values to only include active classes\n",
    "        y_true_filtered = y_true[:, active_classes]\n",
    "        y_pred_filtered = y_pred[:, active_classes]\n",
    "        \n",
    "        report = classification_report(\n",
    "            y_true_filtered, \n",
    "            y_pred_filtered,\n",
    "            target_names=active_labels,\n",
    "            digits=2,\n",
    "            zero_division=0\n",
    "        )\n",
    "        print(\"\\nClassification Report (Active Classes Only):\\n\")\n",
    "        print(report)\n",
    "    \n",
    "    return {\n",
    "        \"Hamming Loss\": hamming,\n",
    "        \"Macro F1\": macro_f1,\n",
    "        \"Micro F1\": micro_f1,\n",
    "        \"Subset Accuracy\": subset_accuracy\n",
    "    }\n",
    "\n",
    "def create_weighted_sampler(labels):\n",
    "    \"\"\"Create a weighted sampler to handle class imbalance.\"\"\"\n",
    "    label_counts = np.sum(labels, axis=0)\n",
    "    weights = 1.0 / label_counts\n",
    "    weights = np.nan_to_num(weights, nan=1.0, posinf=1.0)\n",
    "    sample_weights = np.sum(labels * weights, axis=1)\n",
    "    return WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "def prepare_data(df, all_narratives, all_subnarratives):\n",
    "    \"\"\"Prepare separate datasets for narratives and subnarratives.\"\"\"\n",
    "    narrative_labels = df[\"narratives\"].apply(\n",
    "        lambda x: [1 if n in x else 0 for n in all_narratives]\n",
    "    ).tolist()\n",
    "    \n",
    "    subnarrative_labels = df[\"subnarratives\"].apply(\n",
    "        lambda x: [1 if sn in x else 0 for sn in all_subnarratives]\n",
    "    ).tolist()\n",
    "    \n",
    "    df[\"narrative_labels\"] = narrative_labels\n",
    "    df[\"subnarrative_labels\"] = subnarrative_labels\n",
    "    \n",
    "    return df\n",
    "\n",
    "class CustomBCEWithLogitsLoss(nn.Module):\n",
    "    \"\"\"Custom loss function with label-dependent weighting.\"\"\"\n",
    "    def __init__(self, pos_weight=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, logits, target):\n",
    "        # Calculate label frequencies in this batch\n",
    "        batch_pos_counts = torch.sum(target, dim=0)\n",
    "        batch_neg_counts = target.size(0) - batch_pos_counts\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        eps = 1e-7\n",
    "        batch_weights = (batch_neg_counts + eps) / (batch_pos_counts + eps)\n",
    "        \n",
    "        if self.pos_weight is not None:\n",
    "            batch_weights = batch_weights * self.pos_weight\n",
    "            \n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight=batch_weights, reduction=self.reduction)\n",
    "        return loss_fn(logits, target)\n",
    "\n",
    "def prepare_improved_data(df, all_narratives, all_subnarratives):\n",
    "    \"\"\"Prepare data with improved label handling.\"\"\"\n",
    "    # Create binary label matrices\n",
    "    mlb_narrative = MultiLabelBinarizer()\n",
    "    mlb_subnarrative = MultiLabelBinarizer()\n",
    "    \n",
    "    narrative_labels = mlb_narrative.fit_transform(df[\"narratives\"])\n",
    "    subnarrative_labels = mlb_subnarrative.fit_transform(df[\"subnarratives\"])\n",
    "    \n",
    "    # Store the label classes\n",
    "    narrative_classes = list(mlb_narrative.classes_)\n",
    "    subnarrative_classes = list(mlb_subnarrative.classes_)\n",
    "    \n",
    "    print(f\"Number of narrative classes: {len(narrative_classes)}\")\n",
    "    print(f\"Number of subnarrative classes: {len(subnarrative_classes)}\")\n",
    "    \n",
    "    # Calculate class weights\n",
    "    narrative_weights = compute_class_weights(narrative_labels)\n",
    "    subnarrative_weights = compute_class_weights(subnarrative_labels)\n",
    "    \n",
    "    # Add labels to dataframe\n",
    "    df[\"narrative_labels\"] = list(narrative_labels)\n",
    "    df[\"subnarrative_labels\"] = list(subnarrative_labels)\n",
    "    \n",
    "    return (df, narrative_weights, subnarrative_weights, \n",
    "            narrative_classes, subnarrative_classes)\n",
    "\n",
    "def compute_class_weights(labels):\n",
    "    \"\"\"Compute balanced class weights.\"\"\"\n",
    "    pos_counts = np.sum(labels, axis=0)\n",
    "    neg_counts = len(labels) - pos_counts\n",
    "    \n",
    "    # Balanced weight calculation\n",
    "    weights = neg_counts / (pos_counts + 1e-7)\n",
    "    weights = np.clip(weights, 0.1, 10.0)  # Clip weights to prevent extreme values\n",
    "    \n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "def create_stratified_splits_with_target_lang(df, test_size=0.1, val_size=0.1, \n",
    "                                            train_target_ratio=0.6, random_state=42, \n",
    "                                            target_lang='EN'):\n",
    "    \"\"\"\n",
    "    Create train/val/test splits with controlled amount of target language in training.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing all data\n",
    "        test_size: Proportion of target language data for testing\n",
    "        val_size: Proportion of target language data for validation\n",
    "        train_target_ratio: Proportion of target language data to include in training\n",
    "        random_state: Random seed for reproducibility\n",
    "        target_lang: Target language code (default: 'EN')\n",
    "    \"\"\"\n",
    "    # Separate target language data and other languages\n",
    "    target_lang_df = df[df['language'] == target_lang]\n",
    "    other_langs_df = df[df['language'] != target_lang]\n",
    "    \n",
    "    # Calculate sizes for target language splits\n",
    "    total_target = len(target_lang_df)\n",
    "    test_samples = int(total_target * test_size)\n",
    "    val_samples = int(total_target * val_size)\n",
    "    train_samples = int(total_target * train_target_ratio)\n",
    "    \n",
    "    # Split target language data\n",
    "    # First split out the test set\n",
    "    remaining_target_df, test_df = train_test_split(\n",
    "        target_lang_df,\n",
    "        test_size=test_samples,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Then split the remaining data into train and validation\n",
    "    train_target_df, val_df = train_test_split(\n",
    "        remaining_target_df,\n",
    "        test_size=val_samples,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # If we want more training samples, take them from what's left\n",
    "    if len(train_target_df) > train_samples:\n",
    "        train_target_df = train_target_df.sample(n=train_samples, random_state=random_state)\n",
    "    \n",
    "    # Combine target language training data with other languages\n",
    "    train_df = pd.concat([other_langs_df, train_target_df])\n",
    "    \n",
    "    # Shuffle the training data\n",
    "    train_df = train_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\nData split sizes:\")\n",
    "    print(f\"\\nTraining set:\")\n",
    "    print(f\"Total samples: {len(train_df)}\")\n",
    "    print(\"Language distribution:\")\n",
    "    print(train_df['language'].value_counts())\n",
    "    print(f\"\\nValidation set ({target_lang} only):\")\n",
    "    print(f\"Total samples: {len(val_df)}\")\n",
    "    print(f\"\\nTest set ({target_lang} only):\")\n",
    "    print(f\"Total samples: {len(test_df)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def analyze_rare_labels(df):\n",
    "    \"\"\"Analyze and print information about rare labels in the dataset\"\"\"\n",
    "    narrative_counts = {}\n",
    "    subnarrative_counts = {}\n",
    "    \n",
    "    # Count occurrences of each label\n",
    "    for narratives in df[\"narratives\"]:\n",
    "        for n in narratives:\n",
    "            narrative_counts[n] = narrative_counts.get(n, 0) + 1\n",
    "    \n",
    "    for subnarratives in df[\"subnarratives\"]:\n",
    "        for sn in subnarratives:\n",
    "            subnarrative_counts[sn] = subnarrative_counts.get(sn, 0) + 1\n",
    "    \n",
    "    # Find rare labels\n",
    "    rare_narratives = {k: v for k, v in narrative_counts.items() if v <= 2}\n",
    "    rare_subnarratives = {k: v for k, v in subnarrative_counts.items() if v <= 2}\n",
    "    \n",
    "    print(\"\\nRare Label Analysis:\")\n",
    "    print(f\"Total unique narratives: {len(narrative_counts)}\")\n",
    "    print(f\"Rare narratives (<=2 occurrences): {len(rare_narratives)}\")\n",
    "    print(\"Rare narrative counts:\", rare_narratives)\n",
    "    \n",
    "    print(f\"\\nTotal unique subnarratives: {len(subnarrative_counts)}\")\n",
    "    print(f\"Rare subnarratives (<=2 occurrences): {len(rare_subnarratives)}\")\n",
    "    print(\"Rare subnarrative counts:\", rare_subnarratives)\n",
    "    \n",
    "    return rare_narratives, rare_subnarratives\n",
    "\n",
    "def optimize_threshold(model, val_loader, device, thresholds):\n",
    "    \"\"\"Find the optimal threshold for classification.\"\"\"\n",
    "    model.eval()\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            all_logits.append(outputs.logits)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    logits = torch.cat(all_logits, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        preds = (probs > threshold).float()\n",
    "        f1 = f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "def prepare_data_for_split(df, narrative_classes, subnarrative_classes):\n",
    "    \"\"\"\n",
    "    Prepare validation or test split using the classes from training data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the split data\n",
    "        narrative_classes: List of narrative classes from training data\n",
    "        subnarrative_classes: List of subnarrative classes from training data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with encoded labels matching training data structure\n",
    "    \"\"\"\n",
    "    # Encode narrative labels using training classes\n",
    "    narrative_labels = df[\"narratives\"].apply(\n",
    "        lambda x: [1 if n in x else 0 for n in narrative_classes]\n",
    "    ).tolist()\n",
    "    \n",
    "    # Encode subnarrative labels using training classes\n",
    "    subnarrative_labels = df[\"subnarratives\"].apply(\n",
    "        lambda x: [1 if sn in x else 0 for sn in subnarrative_classes]\n",
    "    ).tolist()\n",
    "    \n",
    "    # Add encoded labels to dataframe\n",
    "    df = df.copy()\n",
    "    df[\"narrative_labels\"] = narrative_labels\n",
    "    df[\"subnarrative_labels\"] = subnarrative_labels\n",
    "      \n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_global_score(narrative_preds, narrative_true, subnarrative_preds, subnarrative_true, \n",
    "                        narrative_weight=0.4, subnarrative_weight=0.6):\n",
    "    \"\"\"\n",
    "    Compute a weighted global score combining narrative and subnarrative predictions.\n",
    "    \n",
    "    Args:\n",
    "        narrative_preds: Predictions for narratives\n",
    "        narrative_true: True labels for narratives\n",
    "        subnarrative_preds: Predictions for subnarratives\n",
    "        subnarrative_true: True labels for subnarratives\n",
    "        narrative_weight: Weight for narrative metrics (default: 0.4)\n",
    "        subnarrative_weight: Weight for subnarrative metrics (default: 0.6)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Combined metrics and individual metrics for both levels\n",
    "    \"\"\"\n",
    "    # Compute metrics for narratives\n",
    "    narrative_metrics = {\n",
    "        \"hamming\": hamming_loss(narrative_true, narrative_preds),\n",
    "        \"macro_f1\": f1_score(narrative_true, narrative_preds, average='macro', zero_division=0),\n",
    "        \"micro_f1\": f1_score(narrative_true, narrative_preds, average='micro', zero_division=0),\n",
    "        \"subset_acc\": (narrative_true == narrative_preds).all(axis=1).mean()\n",
    "    }\n",
    "    \n",
    "    # Compute metrics for subnarratives\n",
    "    subnarrative_metrics = {\n",
    "        \"hamming\": hamming_loss(subnarrative_true, subnarrative_preds),\n",
    "        \"macro_f1\": f1_score(subnarrative_true, subnarrative_preds, average='macro', zero_division=0),\n",
    "        \"micro_f1\": f1_score(subnarrative_true, subnarrative_preds, average='micro', zero_division=0),\n",
    "        \"subset_acc\": (subnarrative_true == subnarrative_preds).all(axis=1).mean()\n",
    "    }\n",
    "    \n",
    "    # Compute global metrics\n",
    "    global_metrics = {\n",
    "        \"global_hamming\": (\n",
    "            narrative_weight * narrative_metrics[\"hamming\"] +\n",
    "            subnarrative_weight * subnarrative_metrics[\"hamming\"]\n",
    "        ),\n",
    "        \"global_macro_f1\": (\n",
    "            narrative_weight * narrative_metrics[\"macro_f1\"] +\n",
    "            subnarrative_weight * subnarrative_metrics[\"macro_f1\"]\n",
    "        ),\n",
    "        \"global_micro_f1\": (\n",
    "            narrative_weight * narrative_metrics[\"micro_f1\"] +\n",
    "            subnarrative_weight * subnarrative_metrics[\"micro_f1\"]\n",
    "        ),\n",
    "        \"global_subset_acc\": (\n",
    "            narrative_weight * narrative_metrics[\"subset_acc\"] +\n",
    "            subnarrative_weight * subnarrative_metrics[\"subset_acc\"]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Compute hierarchical accuracy (both levels must be correct)\n",
    "    hierarchical_accuracy = (\n",
    "        (narrative_true == narrative_preds).all(axis=1) &\n",
    "        (subnarrative_true == subnarrative_preds).all(axis=1)\n",
    "    ).mean()\n",
    "    \n",
    "    # Combine all metrics\n",
    "    all_metrics = {\n",
    "        \"narrative_metrics\": narrative_metrics,\n",
    "        \"subnarrative_metrics\": subnarrative_metrics,\n",
    "        \"global_metrics\": global_metrics,\n",
    "        \"hierarchical_accuracy\": hierarchical_accuracy\n",
    "    }\n",
    "    \n",
    "    return all_metrics\n",
    "\n",
    "def evaluate_global_performance(narrative_model, subnarrative_model, val_narrative_loader, val_subnarrative_loader, device, \n",
    "                                narrative_threshold=0.3, subnarrative_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Evaluate both models together on the validation set.\n",
    "    \"\"\"\n",
    "    # Get predictions for both levels\n",
    "    narrative_preds, narrative_true = get_predictions(\n",
    "        narrative_model, val_narrative_loader, device, narrative_threshold)\n",
    "    subnarrative_preds, subnarrative_true = get_predictions(\n",
    "        subnarrative_model, val_subnarrative_loader, device, subnarrative_threshold)\n",
    "    \n",
    "    # Compute global scores\n",
    "    global_scores = compute_global_score(\n",
    "        narrative_preds, narrative_true,\n",
    "        subnarrative_preds, subnarrative_true\n",
    "    )\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nGlobal Performance Metrics:\")\n",
    "    print(\"\\nNarrative Level Metrics:\")\n",
    "    for metric, value in global_scores[\"narrative_metrics\"].items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nSubnarrative Level Metrics:\")\n",
    "    for metric, value in global_scores[\"subnarrative_metrics\"].items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nGlobal Combined Metrics:\")\n",
    "    for metric, value in global_scores[\"global_metrics\"].items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nHierarchical Accuracy: {global_scores['hierarchical_accuracy']:.4f}\")\n",
    "    \n",
    "    return global_scores\n",
    "\n",
    "def train_model_improved(model, train_loader, val_loader, device, task_type, \n",
    "                        narrative_classes, subnarrative_classes, class_weights, num_epochs=5):\n",
    "    \"\"\"Improved training function with better handling of multilabel classification.\"\"\"\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "    criterion = CustomBCEWithLogitsLoss(pos_weight=class_weights.to(device))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', \n",
    "                                                         factor=0.5, patience=2)\n",
    "    \n",
    "    best_f1 = 0\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    # Variables for threshold adjustment\n",
    "    thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm.tqdm(train_loader, desc=\"Training\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits, inputs['labels'])\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Find best threshold on validation set\n",
    "        best_threshold = optimize_threshold(model, val_loader, device, thresholds)\n",
    "        \n",
    "        y_pred, y_true = get_predictions(model, val_loader, device, threshold=best_threshold)\n",
    "        class_labels = narrative_classes if task_type == 'narrative' else subnarrative_classes\n",
    "        val_metrics = evaluate_model(y_pred, y_true, class_labels, print_report=True)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"Best threshold: {best_threshold:.2f}\")\n",
    "        for metric, value in val_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Update learning rate based on F1 score\n",
    "        scheduler.step(val_metrics['Macro F1'])\n",
    "        \n",
    "        current_f1 = val_metrics['Macro F1']\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'best_threshold': best_threshold,\n",
    "                'best_f1': best_f1\n",
    "            }, f'best_{task_type}_model.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch + 1}\")\n",
    "                break\n",
    "    \n",
    "    return model, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract datasets\n",
    "extract_datasets()\n",
    "\n",
    "# Load labels and get unique narratives and subnarratives\n",
    "labels_df, all_narratives, all_subnarratives = load_and_map_labels(LABELS_PATH)\n",
    "\n",
    "print(f\"Found {len(all_narratives)} unique narratives and {len(all_subnarratives)} unique subnarratives\")\n",
    "\n",
    "# Map inputs to labels\n",
    "article_ids = labels_df[\"article_id\"]\n",
    "df = map_input_to_label_with_lang(INPUTS_PATH, article_ids, labels_df)\n",
    "\n",
    "# Remove excess \"Other\" labels\n",
    "other_df = df[\n",
    "    df[\"narratives\"].apply(lambda x: any(\"Other\" in item for item in x)) & \n",
    "    df[\"subnarratives\"].apply(lambda x: any(\"Other\" in item for item in x))\n",
    "].sample(frac=0.7, random_state=42)\n",
    "df = df.drop(other_df.index)\n",
    "\n",
    "# Analyze rare labels before splitting\n",
    "print(\"Analyzing label distribution before splitting...\")\n",
    "rare_narratives, rare_subnarratives = analyze_rare_labels(df)\n",
    "\n",
    "# Create splits with modified strategy\n",
    "train_df, val_df, test_df = create_stratified_splits_with_target_lang(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    val_size=0.1,\n",
    "    train_target_ratio=0.6,  \n",
    "    random_state=42,\n",
    "    target_lang='EN'\n",
    ")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Prepare training data and get class information\n",
    "train_data, narrative_weights, subnarrative_weights, narrative_classes, subnarrative_classes = prepare_improved_data(\n",
    "    train_df, all_narratives, all_subnarratives)\n",
    "\n",
    "# Prepare validation and test data using training classes\n",
    "val_data = prepare_data_for_split(val_df, narrative_classes, subnarrative_classes)\n",
    "test_data = prepare_data_for_split(test_df, narrative_classes, subnarrative_classes)\n",
    "\n",
    "# Create datasets\n",
    "train_narrative_dataset = NarrativeDataset(train_data, tokenizer, max_len=512, task_type='narrative')\n",
    "val_narrative_dataset = NarrativeDataset(val_data, tokenizer, max_len=512, task_type='narrative')\n",
    "test_narrative_dataset = NarrativeDataset(test_data, tokenizer, max_len=512, task_type='narrative')\n",
    "\n",
    "train_subnarrative_dataset = NarrativeDataset(train_data, tokenizer, max_len=512, task_type='subnarrative')\n",
    "val_subnarrative_dataset = NarrativeDataset(val_data, tokenizer, max_len=512, task_type='subnarrative')\n",
    "test_subnarrative_dataset = NarrativeDataset(test_data, tokenizer, max_len=512, task_type='subnarrative')\n",
    "\n",
    "# Create weighted samplers for training\n",
    "narrative_sampler = create_weighted_sampler(train_data[\"narrative_labels\"].tolist())\n",
    "subnarrative_sampler = create_weighted_sampler(train_data[\"subnarrative_labels\"].tolist())\n",
    "\n",
    "# Create data loaders\n",
    "train_narrative_loader = DataLoader(\n",
    "    train_narrative_dataset, \n",
    "    batch_size=16,\n",
    "    sampler=narrative_sampler,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_narrative_loader = DataLoader(\n",
    "    val_narrative_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_narrative_loader = DataLoader(\n",
    "    test_narrative_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "train_subnarrative_loader = DataLoader(\n",
    "    train_subnarrative_dataset,\n",
    "    batch_size=16,\n",
    "    sampler=subnarrative_sampler,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_subnarrative_loader = DataLoader(\n",
    "    val_subnarrative_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_subnarrative_loader = DataLoader(\n",
    "    test_subnarrative_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models with correct number of labels\n",
    "# Initialize models with correct number of labels\n",
    "narrative_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=len(narrative_classes),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ").to(device)\n",
    "\n",
    "subnarrative_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=len(subnarrative_classes),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ").to(device)\n",
    "\n",
    "# Train models with correct class labels\n",
    "print(\"\\nTraining Narrative Model...\")\n",
    "narrative_model, narrative_threshold = train_model_improved(\n",
    "    narrative_model,\n",
    "    train_narrative_loader,\n",
    "    val_narrative_loader,\n",
    "    device,\n",
    "    'narrative',\n",
    "    narrative_classes,\n",
    "    subnarrative_classes,\n",
    "    narrative_weights,\n",
    "    num_epochs=5\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Subnarrative Model...\")\n",
    "subnarrative_model, subnarrative_threshold = train_model_improved(\n",
    "    subnarrative_model,\n",
    "    train_subnarrative_loader,\n",
    "    val_subnarrative_loader,\n",
    "    device,\n",
    "    'subnarrative',\n",
    "    narrative_classes,\n",
    "    subnarrative_classes,\n",
    "    subnarrative_weights,\n",
    "    num_epochs=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate global performance on validation set\n",
    "print(\"\\nEvaluating global performance on validation set...\")\n",
    "val_global_scores = evaluate_global_performance(\n",
    "    narrative_model, subnarrative_model,\n",
    "    val_narrative_loader, val_subnarrative_loader,\n",
    "    device\n",
    ")\n",
    "\n",
    "# Evaluate global performance on test set\n",
    "print(\"\\nEvaluating global performance on test set...\")\n",
    "test_global_scores = evaluate_global_performance(\n",
    "    narrative_model, subnarrative_model,\n",
    "    test_narrative_loader, test_subnarrative_loader,\n",
    "    device,\n",
    "    narrative_threshold=narrative_threshold,\n",
    "    subnarrative_threshold=subnarrative_threshold\n",
    ")\n",
    "\n",
    "\n",
    "# Save final results\n",
    "results = {\n",
    "    'validation_scores': val_global_scores,\n",
    "    'test_scores': test_global_scores,\n",
    "    'narrative_threshold': narrative_threshold,\n",
    "    'subnarrative_threshold': subnarrative_threshold,\n",
    "    'model_parameters': {\n",
    "        'narrative_labels': len(narrative_classes),\n",
    "        'subnarrative_labels': len(subnarrative_classes)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\nFinal Performance Summary:\")\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(f\"Global Macro F1: {val_global_scores['global_metrics']['global_macro_f1']:.4f}\")\n",
    "print(f\"Hierarchical Accuracy: {val_global_scores['hierarchical_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"Global Macro F1: {test_global_scores['global_metrics']['global_macro_f1']:.4f}\")\n",
    "print(f\"Hierarchical Accuracy: {test_global_scores['hierarchical_accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
